\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}
\pgfmathdeclarefunction{circle}{1}{%
\pgfmathparse{2/pi*sqrt(1 - #1^2)}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\F{\mathcal{F}}
\newcommand\eps{\varepsilon}
\newcommand\one{\mathbbm{1}}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\title{Лекция 13. Мартингалы (продолжение). Процесс Бернулли}

\begin{document}
\maketitle

\section{Декомпозиция Дуба}

Важным моментом, касающимся предсказуемых с.в. является декомпозиция Дуба.
\begin{theorem}[Декомпозиция Дуба]
  Пусть $X_n$ --- субмартингал. Тогда он может быть представлен как $X_n = M_n + H_n$, где $M_n$ --- мартингал, а $H_n$ --- предсказуемая неубывающая последовательность.
\end{theorem} 
\begin{proof}
  Возьмем $A_0 = 0$ и каждый следующий $A_n$ такой, что
  \begin{align*}
    A_{n + 1} - A_n = E[X_{n + 1} \mid \F_n] - X_n
  \end{align*}
  Правая часть равенства измерима по $\F_n$, значит, по индукции можно доказать, что $A_{n + 1}$ измерима по $\F_n$, то есть является предсказуемой. Также заметим, что правая часть неотрицательна (так как $X_n$ --- субмартингал), а значит, $A_n$ --- возрастающая последовательность.

  Остается показать, что $M_n$ --- мартингал, то есть
  \begin{align*}
    E[M_n \mid \F_{n - 1}] &= E[X_n - A_n \mid \F_{n - 1}] \\
    &= E[X_n \mid \F_{n -1}] - A_n = X_{n - 1} - A_{n - 1} = M_{n - 1}.
  \end{align*}
\end{proof}

\section{Ветвящиеся процессы}

Ветвящиеся процессы в общем случае это процессы, описывающие бесполое размножение каких-то организмов с помощью генеалогических деревьев. Одним из простейших примеров является следующий процесс, называемый процессом Гальтона-Ватсона.

Пусть у нас есть двумерная последовательность независимых, одинаково распределенных с.в. $X_i^n \in \N$. И также есть процесс $\{Z_n\}_{n \in \N}$, описываемый следующим образом: $Z_0 = 1$ и
\begin{align*}
  Z_{n + 1} = \begin{cases}
    \sum_{i = 1}^{Z_n} X_i^{n + 1}, \text{ if } Z_n > 0,
    0, \text{else}. 
  \end{cases}
\end{align*}
В данном случае $X_i^n$ --- это число потомков, которых породит $i$-ая особь из поколения $n$, если она вообще будет существовать. А сам $Z_n$ отображает размер поколения.

\begin{lemma}
  Пусть $\F_n = \sigma(X_i^m: i \ge 1, 1 \le m \le n)$ (фильтрация, знающая все про первые $n$ поколений). И пусть $\mu = EX_i^n$. Тогда $\frac{Z_n}{\mu^n}$ мартингал относительно фильтрации $\F_n$. 
\end{lemma}
\begin{proof}
  \begin{align*}
    E[Z_{n + 1} \mid \F_n] = E[X_1^{n + 1} + \dots + X_k^{n + 1} \mid \F_n] = k\mu = \mu Z_n.
  \end{align*}

  Поэтому $E[\frac{Z_{n + 1}}{\mu^{n + 1}} \mid \F_n] = \frac{Z_n}{\mu_n}$.
\end{proof}

Отсюда понятно, что $Z_{n}$ ведет себя примерно как $\mu^n$, откуда следуют следующие наблюдения.

\begin{lemma}
  Если $\mu < 1$, то $Z_n = 0$ начиная с какого-то $n$ почти наверное, то есть $\frac{Z_n}{\mu^n} \to 0$.
\end{lemma}
\begin{proof}
  \begin{align*}
    \Pr[Z_n > 0] \le E[Z_n] = \mu^n \to 0.
  \end{align*}
\end{proof}

\begin{lemma}
  Если $\mu = 1$ и $\Pr[X_i^n = 1] < 1$, тогда $Z_n = 0$ начиная с какого-то $n$ почти наверное.
\end{lemma}
\begin{proof}
  $Z_n$ является мартингалом, значит, по теореме о сходимости он сходится к какой-то с.в. Так как $Z_n$ целочисленна, то и предел целочисленный.

  Пусть этот предел не ноль. Тогда 
  \begin{align*}
    \Pr[Z_n = k \text{ for all } n \ge N] = 0,
  \end{align*}
  поэтому единственный вариант предела есть ноль.
\end{proof}

Для следующей леммы мы введем понятие производящей функции. Пусть $\phi(s) = \sum_{k \ge 0} p_k s^k$, где $p_k = \Pr[X_i^n = k]$

\begin{lemma}
  Если $\mu > 1$ и $Z_0 = 1$, тогда $\Pr[Z_n = 0 \text{ начиная с какого-то }N] = \rho$, где $\rho$ --- единственное решение уравнения $\phi(\rho) = \rho \in [0, 1)$.   
\end{lemma}

Оставим без доказательства (слишком сложно).

\section{Полезные неравенства}

\subsection{Неравенство Азумы}
\begin{theorem}
  Пусть $\{X_n\}_{n \in \N}$ --- супермартингал. Пусть также его изменение ограничено, то есть существует такая последовательность $\{c_n\}_{n \in \N}$, что для любого $n$ верно, что $|X_{n + 1} - X_n| \le c_n$. Тогда для любой $\delta > 0$ верно, что
  \begin{align*}
    \Pr[X_n - X_0 \ge \delta] \le \exp\left(-\frac{\delta^2}{2 \sum_{i = 1}^{n - 1}c_i}\right).
  \end{align*}
\end{theorem}

Очень легко переделать это в неравенство для субмартингала и в двустороннее неравенство для мартингала:

\begin{theorem}
  Пусть $\{X_n\}_{n \in \N}$ --- субмартингал. Пусть также его изменение ограничено, то есть существует такая последовательность $\{c_n\}_{n \in \N}$, что для любого $n$ верно, что $|X_{n + 1} - X_n| \le c_n$. Тогда для любой $\delta > 0$ верно, что
  \begin{align*}
    \Pr[X_n - X_0 \le -\delta] \le \exp\left(-\frac{\delta^2}{2 \sum_{i = 1}^{n - 1}c_i}\right).
  \end{align*}
\end{theorem}

\begin{theorem}
  Пусть $\{X_n\}_{n \in \N}$ --- мартингал. Пусть также его изменение ограничено, то есть существует такая последовательность $\{c_n\}_{n \in \N}$, что для любого $n$ верно, что $|X_{n + 1} - X_n| \le c_n$. Тогда для любой $\delta > 0$ верно, что
  \begin{align*}
    \Pr[|X_n - X_0| \ge \delta] \le 2\exp\left(-\frac{\delta^2}{2 \sum_{i = 1}^{n - 1}c_i}\right).
  \end{align*}
\end{theorem}

Данное неравенство доказывается через общие границы Чернова, примененные к $X_n - X_0 = \sum_{i = 1}^{n - 1} (X_{i + 1} - X_i)$, затем выделению каждого члена суммы в отдельный множитель, который ограничивается через лемму Хёффдинга. 

\subsection{Неравенство МакДиармида}

\begin{theorem}
  Пусть $X_1, \dots, X_n$ --- независимые с.в., определенные на $\Omega_1, \dots, \Omega_n$ соответственно. Пусть $\Omega = \Omega_1 \times \dots \times \Omega_n$. Пусть также есть функция $f: \Omega \to \R$, для которой существует такой набор чисел $c_1, \dots, c_n$, что для любых двух векторов элементарных исходов $\omega, \nu \in \Omega$, отличных только в $i$-ом компоненте верно, что $|f(\omega) - f(\nu)| \le c_i$. Пусть также $X = f(X_1, \dots, X_n)$. Тогда
  \begin{align*}
    \Pr[X - E[X] \ge \lambda] &\le \exp\left(-\frac{2\delta^2}{\sum_{i = 1}^{n - 1}c_i}\right), \\
    \Pr[X - E[X] \le -\lambda] &\le \exp\left(-\frac{2\delta^2}{\sum_{i = 1}^{n - 1}c_i}\right). \\
  \end{align*}
\end{theorem}

\section{Процесс Бернулли}

После сложных мартингалов мы рассмотрим более простой процесс Бернулли (для подготовки к рассмотрению более сложного процесса Пуассона). Процессом Бернулли называется случайный процесс $\{X_t\}_{t \in \N}$, в котором все $X_t$ независимы и следуют одинаковому распределению Бернулли с одинаковым параметром $p$. С его помощью часто моделируют разные реальные штуки:
\begin{itemize}
  \item Браки на производстве
  \item Прибытие посетителя в магазин в интервалы времен
  \item Поступление запросов на сервер
\end{itemize}

Как вообще стоит рассматривать случайный процесс, в том числе процесс Бернулли? В случае, когда у нас конечное множество с.в. $\{X_t\}$, то нам достаточно задать совместную функцию вероятности или совместную плотность вероятности для всех возможных подмножеств $\{X_{t_1}, \dots, X_{t_k}\}$. Это необходимо, так как с.в. могут быть и зависимы. Но в случае с бесконечными последовательностями даже если мы определим совместные функции/плотности вероятности на всех конечных подмножествах, мы все равно не сможем определить, например, вероятность того что все $X_t = 1$.

Поэтому в случае со случайными процесами стоит рассматривать $\Omega$ как множество всех возможных последовательностей, которые могут получиться из значений случайных величин. В случае с процессом Бернулли --- это все возможные последовательности из нолей и единиц. В таком случае легко найти вероятность события, что все $X_t = 1$ как в случае, когда $p < 1$:
\begin{align*}
  &\Pr(\forall t\in \N \ X_t = 1) \le \Pr(\forall t\in [1..n] \ X_t = 1) = p^n \\
  \Rightarrow& \Pr(\forall t\in \N \ X_t = 1) = 0,
\end{align*}
так и для $p = 1$:
\begin{align*}
  \Pr(\forall t\in \N \ X_t = 1) \ge 1 - \sum_{t = 1}^{+\infty} \Pr[X_t = 0] = 1.
\end{align*}
В дальнейшем случай с $p = 1$ (как и с $p = 0$) мы рассматривать не будем, так как ничего интересного в этом нет.

\subsection{Свойства процесса Бернулли}
Рассмотрим несколько свойств процесса Бернулли. Напомним, что так как какждый $X_t$ следует распределению Бернулли независимо от других с.в., то
\begin{itemize}
  \item $E[X_t] = p$
  \item $\Var(X_t) = p(1-p)$
\end{itemize}

Рассмотрим следующие две случайные величины.

\textbf{$S_n$ --- число единиц в первые $n$ единиц времени.} Так как $X_t$ независимы, то $S_n \sim \Bin(n, p)$, причем
\begin{itemize}
  \item $\Pr[S_n = k] = \binom{n}{k} p^k (1 - p)^{n - k}$
  \item $E[S_n] = np$
  \item $\Var(S_n) = np(1 - p)$
\end{itemize} 

\textbf{$T_1$ --- время первой единицы.} Так как $X_t$ независимы, то $T_1 \sim \Geom(p)$, причем
\begin{itemize}
  \item $\Pr[T_1 = k] = p (1 - p)^{k - 1}$
  \item $E[T_n] = \frac{1}{p}$
  \item $\Var(T_n) = \frac{1 - p}{p^2}$
\end{itemize} 

\textbf{Беспамятство.} Если мы рассматриваем процесс $Y_t = X_{t + n}$ для какого-то фиксированного $n$, то это опять будет процесс Бернулли, так как:
\begin{itemize}
  \item Все $Y_t$ независимы
  \item Все $Y_t$ имеют распределение $\Bern(p)$
\end{itemize}

Более того, если мы говорим, что $Y_t = X_{t + N}$, где $N$ --- какая-то случайная величина, то в некоторых случаях мы также получаем процесс Бернулли. Например:
\begin{itemize}
  \item Пусть $N$ --- время первого успеха в процессе $X_t$. Тогда все $X_{t + N}$ независимы друг от друга и от $N$, поэтому являются процессом Бернулли.
  \item Пусть теперь $N$ --- время перед первым успехом. Тогда $X_{N + 1}$ уже не независим от $N$, а точно равен единице. 
\end{itemize}
В общем случае $Y_t$ будет процессом Бернулли, если $N$ является временем останова, то есть событие $N \ge n$ зависит только от $X_1, \dots, X_{n - 1}$.

Беспамятство дает нам много других полезных свойств процесса Бернулли.

\textbf{Продолжительность занятого периода.} Рассмотрим сервер, на который в каждую единицу времени приходит или не приходит запрос. Это можно описать процессом Бернулли, где событие ``во время $t$ пришел запрос'' соответсвует $X_t = 1$. Скажем, что сервер занят во время $t$, если в это время на него пришел запрос. Занятый период --- это отрезок $[t_1..t_2]$, такой, что во все $t \in [t_1..t_2]$ сервер занят, но в моменты $t_1 - 1$ и $t_2 + 1$ --- свободен или еще не начался. Нас интересует продолжительность первого занятого периода.

Пусть $T_1$ --- время первого запроса на сервер. Благодаря беспамятству мы можем сказать,что $X_{t + T_1}$ задают новый процесс Бернулли с тем же параметром $p$. В этом процессе первый ноль появится во время $T_2 \sim \Geom(1 - p)$. При этом $T_2$ в таком случае и будет равно длине занятого периода.

\textbf{Время $k$-ого успеха.} Пусть $Y_k$ --- время, в которое на сервер прибыл $k$-ый по счету запрос. Пусть $T_k = Y_k - Y_{k - 1}$, то есть интервал между $k-1$-ым и $k$-ым запросами. При этом $Y_k = T_1 + \dots + T_k$.

Благодаря беспамятству мы можем сказать, что в каждый момент $Y_k$ мы запускаем процесс заново. Поэтому каждый $T_k \sim \Geom(p)$. Отчюда мы сразу можем сказать про $Y_k$, что
\begin{itemize}
  \item $E[Y_k] = \sum_{i = 1}^k E[T_k] = \frac{k}{p}$
  \item $\Var(Y_k) = \sum_{i = 1}^k \Var(T_k) = \frac{k(1 - p)}{p^2}$.
\end{itemize}

Также уже было в разборе контрольной, что функция распределения имеет следующий вид:
\begin{align*}
  p_{Y_k}(t) = \binom{t - 1}{k - 1} p^k (1 - p)^{t - k},
\end{align*}
так как мы точно знаем, что во время $t$ произошел успех, и нам надо расставить остальные $k - 1$ успех по оставшимся $t - 1$ ячейкам времени.

\section{Слияние и разделение процесса Бернулли}

Рассмотрим случай, когда у нас на сервер могут поступать запросы с двух источников. Поступление запросов от каждого из источников описывается процессом Бернулли, причем эти два процесса (назовем их $X_t$ и $Y_t$) независимы. Рассмотрим процесс $Z_t$, где $Z_t$ --- индикаторная с.в. события ``в момент времени $t$ поступил запрос от хотя бы одного источника''. То есть $Z_t = \max\{X_t, Y_t\}$. Заметим, что $Z_t$ не зависит от любого $Z_{s \ne t}$, так как является функцией от двух величин $X_t$ и $Y_t$, не зависящих от любых других $X_{s \ne t}$ и $Y_{s \ne t}$. А также 
\begin{align*}
  \Pr[Z_t = 1] = \Pr[X_t = 1 \cup Y_t = 1] = 1 - \Pr[X_t = 0 \cap Y_t = 0] = p + q - pq,
\end{align*}
где $p$ и $q$ --- параметры процессов $X_t$ и $Y_t$ соответственно. Таким образом, $Z_t$ также является процессом Бернулли с параметром $p + q - pq$. Заметье, что вместо логического ``И'' мы можем применить любую другую операцию к двум поступающим запросам. Например, одновременный запрос от двух источников мы можем считать коллизией, и не считать его за запрос. Тогда $Z_t$ также будет процессом Бернулли, но с другим параметром $p + q - 2pq$.

Мы также можем разделять процесс Бернулли на два. Пусть у нас есть процесс Бернулли $Z_t$ с параметром $p$, описывающий запросы на сервер. Каждый раз, когда на сервер поступает запрос, он решает, в какой из двух выходов его отправить дальше, причем он отправляет его в первый выход с вероятностью $q$  и во второй --- c вероятностью $1 - q$, принимая это решение независимо от всего остального. В таком случае запросы, отправленные в первый выход, $X_t$ являются процессом Бернулли с параметром $pq$. (TODO: доказать независимость и бернулевость всех $X_t$). Запросы на втором выходе $Y_t$ --- тоже, но с параметром $p(1 - q)$. Однако процессы $X_t$ и $Y_t$ не являются независимыми, так как если мы знаем, что $X_t = 1$, то мы точно знаем, что $Y_t = 0$.

\section{Аппроксимация процесса Бернулли}

Рассмотрим процесс Бернулли с параметром $p$. Чему равна вероятность того, что за время $n$ будет $k$ успехов? По тому, что мы считали, это будет 
\begin{align*}
  p_X(k) = \binom{n}{k} p^k (1 - p)^{n - k}.
\end{align*}

Но что будет, если мы раздробим время на меньше интервалы (при этом вероятность успеха в каждый интервал будет падать обратно пропорционально длине интервала)? То есть для каждого $n$ мы будем брать $p = {\lambda}{n}$ для какой-то фиксированной $\lambda$ и устремим $n \to \infty$. В таком случае будет:

\begin{align*}
  p_X(k) &= \binom{n}{k} p^k (1 - p)^{n - k} \\
         &= \frac{n(n - 1)\dots(n - k+ 1)}{k!} \cdot \frac{\lambda^k}{n^k} \left(1 - \frac{\lambda}{n}\right)^{n - k} \\
         &= \frac{n}{n} \cdot \frac{n - 1}{n} \cdot \ldots \cdot \frac{n - k + 1}{n} \cdot \frac{\lambda^k}{k!} \left(1 - \frac{\lambda}{n}\right)^n \left(1 - \frac{\lambda}{n}\right)^{-k} \\
         &\to 1 \cdot 1 \cdot \ldots \cdot 1 \cdot \frac{\lambda^k}{k!}e^{-\lambda} \cdot 1 = \frac{\lambda^k}{k!}e^{-\lambda}.
\end{align*}

И это подводит нас к процессу Пуассона, который мы изучим на следующей паре.

\end{document}