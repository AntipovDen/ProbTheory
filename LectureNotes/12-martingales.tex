\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}
\pgfmathdeclarefunction{circle}{1}{%
\pgfmathparse{2/pi*sqrt(1 - #1^2)}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\F{\mathcal{F}}
\newcommand\eps{\varepsilon}
\newcommand\one{\mathbbm{1}}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\title{Лекция 12. Случайные процессы. Мартингалы}

\begin{document}
\maketitle

В остатке курса мы будем изучать случайные процессы. Вообще говоря, случайным процессом называется любое не менее, чем счетное, упорядоченное множество случайных величин $\{X_t\}$. При этом с.в. в этом множестве могут зависеть или не зависеть друг от друга. Индекс $t$ чаще всего интерпретируется как время, и в большинстве случаев принимает либо дискретные значения из $\N$, либо непрерывные из $\R+$. 

Заметьте, что в случае дискретного $t \in \N$ множество $\{X_t\}$ является последовательностью. Мы начнем знакомство со случайными процессами с мартингалов. Очень грубо говоря, это такие процессы, матожидание изменение которых в единицу времени равно нулю. Но сначала нам надо ввести несколько дополнительных понятий.

\section{Условное матожидание}

У нас уже фигурировало матожидание, условное на событии или на другой с.в. В обоих случаях это было обычное матожидание, но с учетом какой-то дополнительной информации, которая у нас есть. Иногда эту информацию ``кодируют'' в $\sigma$-алгебре, и обуславливаются на ней.

Пусть у нас есть вероятностное пространство $(\Omega, \F_0, \Pr)$\footnote{Заметьте, что мы обозначили сигма-алгебру буквой $\F$, а не $\Sigma$, но в этом нет особого скрытого смысла. Просто литература, по которой готовилась лекция, использовала это обозначение.}. Пусть также есть какая-то с.в. $X$ (измеримая и интегрируемая по $\F_0$) и есть какая-то под-алгебра $\F \subset \F_0$. Тогда под $E[X \mid \F]$ мы понимаем любую с.в. $Y$, такую что 

\begin{enumerate}
  \item $Y$ измерима по $\F$
  \item Для любого множества $A \in \F$ выполняется равенство
  \begin{align*}
    \int_A X d\Pr = \int_A Y d\Pr
  \end{align*}
\end{enumerate}

Оно существует и единственно почти наверное. Единственность почти наверное значит, что если какие-то две с.в. $Y_1$ и $Y_2$ удовлетворяют обоим условиям, то $\Pr(Y_1 \ne Y_2) = 0$. Мы не будем доказывать существование и почти единственность, дабы не углубляться в теорию меры.

Как и в других условных вероятностях и матожиданиях, $\F$ в условии стоит воспринимать как имеющуюся у нас информацию. А именно стоит трактовать это так: мы знаем интеграл с.в. на всех множествах из $\F$.

\subsection{Примеры}

\textbf{Пример 1.}

Для того, чтобы понять, как это работает, рассмотрим следующий пример. Пусть у нас дано вероятностной пространство $(\Omega, \F_0, \Pr)$, где $\Omega = \R$, а $\F_0$ --- стандартная $\sigma$-алгебра, построенная на множестве всех полуинтервалов. Пусть $\F$ --- $\sigma$-алгебра, построенная на множестве полуинтервалов с концами в целых числах. Значит, если на исходном пространстве задана какая-то с.в. $X$, то когда мы говорим про $E[X \mid \F]$, мы говорим о какой-то с.в., которая может менять свое значение только в целых числах (иначе она будет не измерима по $\F$). Во всех остальных точках пространства она должна быть константой. На графике изображен пример, когда $X$ --- какая-то непрерывная с.в. (график нормального распределения взят просто для удобства, не путайте, это не плотность вероятности!), а $Y$, обозначенный красным, как раз является $E[X \mid \F]$.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$X(\omega), Y(\omega)$},
    xlabel={$\omega$},
    width = 0.6\textwidth,
    height = 5cm,
    xmax=3,
    xmin=-3,
    % ymax=1.1
    % legend pos = north east
    ]
      \addplot[domain=-3:3, samples=31, draw=blue, ultra thick]{gauss(x, 0, 1)};
      \addplot[draw=red, ultra thick] coordinates {(-3, 0.02) (-2, 0.02)};
      \addplot[draw=red, ultra thick] coordinates {(-2, 0.13) (-1, 0.13)};
      \addplot[draw=red, ultra thick] coordinates {(-1, 0.34) (1, 0.34)};
      \addplot[draw=red, ultra thick] coordinates {(1, 0.13) (2, 0.13)};
      \addplot[draw=red, ultra thick] coordinates {(2, 0.02) (3, 0.02)};
    \end{axis}
  \end{tikzpicture}
\end{center}

\textbf{Пример 2.}

Пусть у нас случайная величина $X$ измерима по $\F$. Это значит, что мы знаем ее значение на любом возможном событии, то есть у нас есть вся информация о ней. Поэтому $E[X \mid \F] = X$.

\textbf{Пример 3.}

Пусть с.в. $X$ независима от $\F$. По определению независимости это значит, что $\F$ не дает никакой информации об $X$. Формально это можно записать так: для любого события $A \in X(\F_0)$ и $B \in \F \subset \F_0$ верно, что
\begin{align*}
  \Pr(\{\omega: X(\omega) \in A\} \cap B) = \Pr(\{\omega: X(\omega) \in A\}) \Pr(B).
\end{align*} 

А так как мы нигде ничего не знаем о с.в., то наше лучшее предположение о с.в. --- это ее среднее значение, то есть $E[X \mid \F] = E[X]$.

\subsection{Свойства условного матожидания}

Свойства все те же, что и у обычного. Мы просто перешли в мир, где нам известна какая-то новая информация.

\begin{itemize}
  \item Линейность: $E[\alpha X + \beta Y \mid \F] = \alpha E[X \mid \F] + \beta E[Y \mid \F]$.
  \item Монотонность: $\forall \omega \ X(\omega) \le Y(\omega) \Rightarrow E[X \mid \F] \le E[Y \mid \F]$.
  \item Неравенство Йенсена: $\phi(x)$ --- вогнута, тогда $\phi(E[X \mid \F]) \le E[\phi(X) \mid \F]$.
\end{itemize}
И пара неочевидных, хотя ничего нового для нас тут нет
\begin{itemize}
  \item Пусть $\F_1 \subset \F_2$. Тогда $E[E(X \mid \F_1) \mid \F_2] = E[E(X \mid \F_2) \mid \F_1] = E[X \mid \F_1]$.
  \item Если $X$ измерима по $\F$, то $E[XY \mid \F] = XE[Y \mid \F]$.
\end{itemize}

\section{Фильтрации}

\emph{Фильтрацией} называется неубывающая последовательность $\sigma$-алгебр в вероятностном пространстве. То есть у нас есть $(\Omega, \F, \Pr)$ и последовательность $\F_1 \subset \F_2 \subset \dots$, причем для любого $n \in \N$ выполняется $\F_n \subset \F$. 

Фильтрацию стоит воспринимать как процесс набора информации, количество которой не убывает со временем. Если дана последовательность с.в. $\{X_n\}_{n \in \N}$, то естественный фильтрацией обычно называют 
\begin{align*}
  \F_n = \sigma(X_1, \dots, X_n),
\end{align*} 
то есть объединение $\sigma$-алгебр, задаваемых первыми $n$ с.в. последовательности. В этом смысле запись $E[X_{n + 1} \mid \F_n]$ означает с.в., равную $X_{n + 1}$, когда нам уже все известно про $X_1, \dots X_n$.

\section{Мартингалы}

Случайный процесс (последовательность с.в. $\{X_n\}_{n \in \N}$) называется \emph{Мартингалом} относительно фильтрации $\{\F_n\}_{n \in \N}$, если выполнены три условия:
\begin{itemize}
  \item Все $X_n$ абсолютно интегрируемы, то есть $E[|X_n|] < \infty$
  \item Для всех $n$ $X_n$ измерима по $\F_n$
  \item Для всех $n$ выполняется $E[X_{n + 1} \mid \F_n] = X_n$
\end{itemize}

Последнее условие иногда записывается так:
\begin{align*}
  E[X_{n + 1} - X_n \mid \F_n] = 0
\end{align*}

Случайный процесс называется \emph{субмартингалом}, если в последнем условии стоит $\ge$. То есть следующая с.в. ожидаемо не меньше предыдущей. Случайный процесс называется \emph{супермартингалом}, если в последнем условии стоит $\le$. Да, казалось бы, что названия должны быть наоборот, но на самом деле ``суб'' относится к текущему наблюдению, которое не превышает ожидание следующего наблюдения $E[X_{t + 1} \mid \F_n]$.

Чаще всего используется естественная фильтрация, которая означает, что мы знаем первые $n$ шагов процесса. В таком случае вместо второго и третьего условия пишут просто:
\begin{align*}
  E[X_{n + 1} \mid X_1, \dots, X_n] = X_n,
\end{align*}
что есть равенство с.в. То есть случайный процесс зависит только от своего прошлого. 

Также возможен случай, когда один случайный процесс $X_n$ является мартингалом относительно другого случайного процесса $Y_n$. Это происходит, когда вместо второго и третьего условия (в определении через фильтрацию) выполняется
\begin{align*}
  E[X_{n + 1} \mid Y_1, \dots, Y_n] = X_n,
\end{align*}

\textbf{Примеры мартингалов}
Самый простой пример: пусть есть последовательность $\{Y_n\}_{n \in \N}$ независимых с.в. с нулевыми матожиданиями, и есть $X_n$, который задается так:
\begin{align*}
  X_n = \begin{cases}
    x, &\text{ if } n = 1, \\
    X_{n - 1} + Y_{n - 1}, &\text{ otherwise.}
  \end{cases}
\end{align*} 
Возьмем фильтрацию $\F_n = \sigma(Y_1, \dots, Y_{n - 1})$. По свойствам условного матожидания легко проверить, что это мартингал:
\begin{align*}
  E[X_{n + 1} \mid \F_n] = E[X_n \mid \F_n] + E[Y_n \mid \F_n] = X_n + E[Y_n] = X_n.
\end{align*}

Второй пример. Пусть есть последовательность $\{Y_n\}_{n \in \N}$ независимых с.в. с единичными матожиданиями, и есть $X_n$, который задается так:
\begin{align*}
  X_n = \prod_{i = 1}^n Y_i,
\end{align*} 
он является мартингалом относительно $\F_n = \sigma(Y_1, \dots, Y_n)$:
\begin{align*}
  E[X_{n + 1} \mid \F_n] = E[X_n Y_{n + 1} \mid \F_n] = X_n E[Y_n \mid \F_n] = X_n E[Y_n] = X_n.
\end{align*}

Пара важных свойств (суб-/супер-)мартингалов:
\begin{lemma}
  Если $X_n$ --- субмартингал относительно $\F_n$, то для любых $m > n$ выполняется $E[X_m \mid \F_n] \ge X_n$

  Если $X_n$ --- супермартингал относительно $\F_n$, то для любых $m > n$ выполняется $E[X_m \mid \F_n] \le X_n$

  Если $X_n$ --- мартингал относительно $\F_n$, то для любых $m > n$ выполняется $E[X_m \mid \F_n] = X_n$
\end{lemma}

\begin{proof}
  Первое доказывается по индукции. Пусть $m = n + k$. Для $k = 1$ утверждение верно по определению. Для больших $k$ воспользуемся свойствами условного матожидания:
  \begin{align*}
    E[X_{n + k} \mid \F_n] = E[E[X_{n + k} \mid \F_{n + k - 1}] \mid \F_n] \ge E[X_{n + k - 1} \mid \F_n].
  \end{align*}

  Для доказательства для супермартингала достаточно заметить, что $-X_n$ является субмартингалом, а для доказательства для мартингала достаточно заметить, что он является и суб-, и супер-мартингалом.
\end{proof}

\begin{lemma}\label{lem:convex}
  Если $X_n$ --- мартингал относительно $\F_n$, а $\phi$ --- вогнутая функция, то $\phi(X_n)$ --- субмартингал. Если $phi$ неубывающая, то достаточно, чтобы $X_n$ был субмартингалом.
\end{lemma}
\begin{proof}
  По неравенству Йенсена в первом случае получаем:
  \begin{align*}
    E[\phi(X_{n + 1}) \mid \F_n] \ge \phi(E[X_{n + 1} \mid \F_n]) = \phi(X_n)
  \end{align*}
  Во втором случае последнее равенство заменяется на неравенство (пользуемся неубыванием $\phi$):
  \begin{align*}
    E[\phi(X_{n + 1}) \mid \F_n] \ge \phi(E[X_{n + 1} \mid \F_n]) \ge \phi(X_n)
  \end{align*}
\end{proof}

\subsection{Предсказуемые с.в.}

Последовательность с.в. $\{H_n\}_{n \in \N}$ называется предсказуемой относительно фильтрации $\{\F_n\}_{n \in \N}$, если для любого $n$ $H_n$ измерима относительно $\F_{n - 1}$.

Классический пример --- ставки игрока в казино. Если нам известна его стратегия, то когда мы знаем результат его первых $n$ ставок, то мы обычно можем сказать, какой будет его $n + 1$-ая ставка. Например, известная стратегия -- удваивать ставку после проигрыша и прекращать игру после первой победы. Такая тактика (при бесконечном имеющимся капитале) гарантирует вам прибыль в размере первой ставки.

Для как раз таких стратегий часто используется следующая последовательность с.в.:
\begin{align*}
  (H \cdot X)_n = \sum_{i = 1}^n H_i (X_i - X_{i - 1}),
\end{align*}
Которая по сути равна балансу игрока, делающего ставки по стратегии $H_i$, где $X_i$ --- выигрыш в $i$-ой игре при условии единичной ставки.

\begin{theorem}
  Если $X_n$ --- супремартингал, а $H_n \ge 0$ --- предсказуемая, ограниченная последовательность с.в. Тогда $(H \cdot X)_n$ --- супермартингал.
\end{theorem}
\begin{proof}
  Так как $(H \cdot X)_n$ измеримо по $\F_n$, а $H_n$ также измеримо по $\F_{n - 1}$,
  \begin{align*}
    E[(X \cdot H)_{n + 1} \mid \F_n] &= (H \cdot X)_n + E[H_{n + 1}(X_{n + 1} - X_n) \mid \F_n] \\
    &= (H \cdot X)_n + H_{n + 1}E[(X_{n + 1} - X_n) \mid \F_n] = (H \cdot X)_n.
  \end{align*}
\end{proof}

\subsection{Ограничение прибыли}

В данном разделе мы рассмотрим upcrossing inequality (ограничение на пересечения вверх/ограничение на прибыль), которое играет важную роль, например, в биржевых торгах. 

Для начала введем определение \emph{времени останова}. Натуральная с.в. $N$ называется временем останова относительно фильтрации $\{\F_n\}$, если для всех $n$ событие $\{N = n\}$ измеримо относительно $\F_n$. Если мы введем индикаторную величину $H_n = [N \ge n]$, то она будет измерима относительно $\F_{n - 1}$ (так как событие $\{N \ge n\} = \{N \le n - 1\}^c \in \F_{n - 1}$), а значит, она будет предсказуема. 

Таким образом, если $X_n$ --- супермартингал, то $(H \cdot X)_n = X_{n \cap N} - X_0$ --- супермартингал. Мы тут обозначили $X_{\min(n, N)} = X_{n \cap N}$ для более короткой записи. Таким образом, и $X_{n \cap N}$ --- супермартингал (как сумма двух супермартингалов).

Введем теперь понятие upcrossing (пересечение вверх). Пусть у нас есть случайный процесс $X_n$, для простоты представим, что это стоимость акций в момент времени $n$. И у нас следующая стратегия: мы покупаем акции, когда их цена становится ниже какого-то $a$ и продаем, когда их цена становится выше какого-то $b > a$. Тогда за время $n$ наша суммарная прибыль будет $(b -a)$ умножить на число случаев, когда цена акций возрастала от $a$ до $b$ (что есть число пересечений полосы $[a, b]$ снизу вверх до времени $n$). Введем также времена, когда мы покупаем и продаем акции:
\begin{align*}
  N_{2k - 1} &= \inf\{m > N_{2k - 2}: X_m \le a\} \text{ (время покупки)}\\
  N_{2k} &= \inf\{m > N_{2k - 1}: X_m \ge b\} \text{ (время продажи)}
\end{align*}
и положим $N_0 = -1$. В таком случае upcrossing --- это время от покупки до продажи (на рисунке они обозначены линиями).

\begin{center}
  \begin{tikzpicture}
    \draw [thick] (0, 0) -- (7, 0) node [pos=0, left] {$a$};
    \draw [thick] (0, 2) -- (7, 2) node [pos=0, left] {$b$};

    \draw [fill=black] (0, 0.5) circle (0.5mm);
    \draw [fill=black] (0.4, -0.2) circle (0.5mm);
    \draw [fill=black] (0.8, 0.2) circle (0.5mm);
    \draw [fill=black] (1.2, 1) circle (0.5mm);
    \draw [fill=black] (1.6, 1.5) circle (0.5mm);
    \draw [fill=black] (2, 2.2) circle (0.5mm);
    \draw [fill=black] (2.4, 1.8) circle (0.5mm);
    \draw [fill=black] (2.8, 0.9) circle (0.5mm);
    \draw [fill=black] (3.2, -0.1) circle (0.5mm);
    \draw [fill=black] (3.6, 0.8) circle (0.5mm);
    \draw [fill=black] (4, 1) circle (0.5mm);
    \draw [fill=black] (4.4, 2.2) circle (0.5mm);
    \draw [fill=black] (4.8, 1.6) circle (0.5mm);
    \draw [fill=black] (5.2, 1.8) circle (0.5mm);
    \draw [fill=black] (5.6, 0.8) circle (0.5mm);
    \draw [fill=black] (6, -0.1) circle (0.5mm);
    \draw [fill=black] (6.4, 0.5) circle (0.5mm);
    \draw [fill=black] (6.8, -0.2) circle (0.5mm);

    \draw (0.4, -0.2) -- (0.8, 0.2) -- (1.2, 1) -- (1.6, 1.5) -- (2, 2.2);
    \draw (3.2, -0.1) -- (3.6, 0.8) -- (4, 1) -- (4.4, 2.2);
    \draw (6, -0.1) -- (6.4, 0.5) -- (6.8, -0.2);

    \node [below] at (0.4, -0.2) {$N_1$};
    \node [above] at (2, 2.2) {$N_2$};
    \node [below] at (3.2, -0.1) {$N_3$};
    \node [above] at (4.4, 2.2) {$N_4$};
    \node [below] at (6, -0.1) {$N_5$};
  \end{tikzpicture}
\end{center}

Введем $H_n$ --- индикаторную величину, показывающую, на руках ли у нас акции в момент времени $n$, или другими словами,
\begin{align*}
  H_n = \begin{cases}
    1, \text{ если }n\text{ лежит между }N_{2k - 1}\text{ и }N_{2k},\\
    0, \text{ иначе}.
  \end{cases}
\end{align*}
Нагонец, введем с.в. $U_n = \sup\{k : N_{2k} \le n\}$ --- число upcrossing'ов, завершенных к моменту времени $n$. Для него известно следующее утверждение
\begin{theorem}[Upcrossing inequality]
  Если $X_m$ --- субмартингал, тогда
  \begin{align*}
    (b - a)E[U_n] \le E[(X_n - a)^+] - E[(X_0 - a)^+]
  \end{align*}
\end{theorem}
\begin{proof}
  Пусть $Y_m = a + (X_m - a)^+$. Это субмартингал по Лемме~\ref{lem:convex}. Он пересекает полосу $[a,b]$ столько же раз, сколько $X_m$ (единственное отличие --- он не уходит ниже $a$). Тогда наша прибыль описывается как $(H \cdot Y)_n \ge (b - a)U_n$, так как в случае с $Y$ последний начатый upcrossing не приносит убытка.

  Пусть теперь $K_m = 1 - H_m$. Тогда $Y_n - Y_0 = (H \cdot Y)_n + (K \cdot Y)_n$. Так как $(K \cdot Y)_n$ -- субмартингал, $(K \cdot Y)_n \ge (K \cdot Y)_0 = 0$. То есть $E[(H \cdot Y)_n] \le E[Y_n - Y_0]$. Остается все подставить и воспользоваться линейностью матожидания.
\end{proof}

Из этого результата следует другой, куда более общий результат о сходимости мартингалов.

\begin{theorem}
  Если $X_n$ --- субмартингал, причем $\sup E[X_n^+] < +\infty$, тогда $X_n$ сходится почти наверное к какому-то $X$ с $E[|X|] < \infty$
\end{theorem}

Мы опустим доказательство из-за его излишней техничности, но в целом оно основано на том, что число upcrossing'ов любого отрезка $[a , b]$ конечно, откуда следует сходимость верхнего и нижнего пределов $X_n$.

\subsection{Декомпозиция Дуба}

Важным моментом, касающимся предсказуемых с.в. является декомпозиция Дуба.
\begin{theorem}[Декомпозиция Дуба]
  Пусть $X_n$ --- субмартингал. Тогда он может быть представлен как $X_n = M_n + H_n$, где $M_n$ --- мартингал, а $H_n$ --- предсказуемая неубывающая последовательность.
\end{theorem} 
\begin{proof}
  Возьмем $A_0 = 0$ и каждый следующий $A_n$ такой, что
  \begin{align*}
    A_{n + 1} - A_n = E[X_{n + 1} \mid \F_n] - X_n
  \end{align*}
  Правая часть равенства измерима по $\F_n$, значит, по индукции можно доказать, что $A_{n + 1}$ измерима по $\F_n$, то есть является предсказуемой. Также заметим, что правая часть неотрицательна (так как $X_n$ --- субмартингал), а значит, $A_n$ --- возрастающая последовательность.

  Остается показать, что $M_n$ --- мартингал, то есть
  \begin{align*}
    E[M_n \mid \F_{n - 1}] &= E[X_n - A_n \mid \F_{n - 1}] \\
    &= E[X_n \mid \F_{n -1}] - A_n = X_{n - 1} - A_{n - 1} = M_{n - 1}.
  \end{align*}
\end{proof}

\section{Ветвящиеся процессы}

Ветвящиеся процессы в общем случае это процессы, описывающие бесполое размножение каких-то организмов с помощью генеалогических деревьев. Одним из простейших примеров является следующий процесс, называемый процессом Гальтона-Ватсона.

Пусть у нас есть двумерная последовательность независимых, одинаково распределенных с.в. $X_i^n \in \N$. И также есть процесс $\{Z_n\}_{n \in \N}$, описываемый следующим образом: $Z_0 = 1$ и
\begin{align*}
  Z_{n + 1} = \begin{cases}
    \sum_{i = 1}^{Z_n} X_i^{n + 1}, \text{ if } Z_n > 0,
    0, \text{else}. 
  \end{cases}
\end{align*}
В данном случае $X_i^n$ --- это число потомков, которых породит $i$-ая особь из поколения $n$, если она вообще будет существовать. А сам $Z_n$ отображает размер поколения.

\begin{lemma}
  Пусть $\F_n = \sigma(X_i^m: i \ge 1, 1 \le m \le n)$ (фильтрация, знающая все про первые $n$ поколений). И пусть $\mu = EX_i^n$. Тогда $\frac{Z_n}{\mu^n}$ мартингал относительно фильтрации $\F_n$. 
\end{lemma}
\begin{proof}
  \begin{align*}
    E[Z_{n + 1} \mid \F_n] = E[X_1^{n + 1} + \dots + X_k^{n + 1} \mid \F_n] = k\mu = \mu Z_n.
  \end{align*}

  Поэтому $E[\frac{Z_{n + 1}}{\mu^{n + 1}} \mid \F_n] = \frac{Z_n}{\mu_n}$.
\end{proof}

Отсюда понятно, что $Z_{n}$ ведет себя примерно как $\mu^n$, откуда следуют следующие наблюдения.

\begin{lemma}
  Если $\mu < 1$, то $Z_n = 0$ начиная с какого-то $n$ почти наверное, то есть $\frac{Z_n}{\mu^n} \to 0$.
\end{lemma}
\begin{proof}
  \begin{align*}
    \Pr[Z_n > 0] \le E[Z_n] = \mu^n \to 0.
  \end{align*}
\end{proof}

\begin{lemma}
  Если $\mu = 1$ и $\Pr[X_i^n = 1] < 1$, тогда $Z_n = 0$ начиная с какого-то $n$ почти наверное.
\end{lemma}
\begin{proof}
  $Z_n$ является мартингалом, значит, по теореме о сходимости он сходится к какой-то с.в. Так как $Z_n$ целочисленна, то и предел целочисленный.

  Пусть этот предел не ноль. Тогда 
  \begin{align*}
    \Pr[Z_n = k \text{ for all } n \ge N] = 0,
  \end{align*}
  поэтому единственный вариант предела есть ноль.
\end{proof}

Для следующей леммы мы введем понятие производящей функции. Пусть $\phi(s) = \sum_{k \ge 0} p_k s^k$, где $p_k = \Pr[X_i^n = k]$

\begin{lemma}
  Если $\mu > 1$ и $Z_0 = 1$, тогда $\Pr[Z_n = 0 \text{ начиная с какого-то }N] = \rho$, где $\rho$ --- единственное решение уравнения $\phi(\rho) = \rho \in [0, 1)$.   
\end{lemma}

Оставим без доказательства (слишком сложно).

\section{Полезные неравенства}

\subsection{Неравенство Азумы}
\begin{theorem}
  Пусть $\{X_n\}_{n \in \N}$ --- супермартингал. Пусть также его изменение ограничено, то есть существует такая последовательность $\{c_n\}_{n \in \N}$, что для любого $n$ верно, что $|X_{n + 1} - X_n| \le c_n$. Тогда для любой $\delta > 0$ верно, что
  \begin{align*}
    \Pr[X_n - X_0 \ge \delta] \le \exp\left(-\frac{\delta^2}{2 \sum_{i = 1}^{n - 1}c_i}\right).
  \end{align*}
\end{theorem}

Очень легко переделать это в неравенство для субмартингала и в двустороннее неравенство для мартингала:

\begin{theorem}
  Пусть $\{X_n\}_{n \in \N}$ --- субмартингал. Пусть также его изменение ограничено, то есть существует такая последовательность $\{c_n\}_{n \in \N}$, что для любого $n$ верно, что $|X_{n + 1} - X_n| \le c_n$. Тогда для любой $\delta > 0$ верно, что
  \begin{align*}
    \Pr[X_n - X_0 \le -\delta] \le \exp\left(-\frac{\delta^2}{2 \sum_{i = 1}^{n - 1}c_i}\right).
  \end{align*}
\end{theorem}

\begin{theorem}
  Пусть $\{X_n\}_{n \in \N}$ --- мартингал. Пусть также его изменение ограничено, то есть существует такая последовательность $\{c_n\}_{n \in \N}$, что для любого $n$ верно, что $|X_{n + 1} - X_n| \le c_n$. Тогда для любой $\delta > 0$ верно, что
  \begin{align*}
    \Pr[|X_n - X_0| \ge \delta] \le 2\exp\left(-\frac{\delta^2}{2 \sum_{i = 1}^{n - 1}c_i}\right).
  \end{align*}
\end{theorem}

Данное неравенство доказывается через общие границы Чернова, примененные к $X_n - X_0 = \sum_{i = 1}^{n - 1} (X_{i + 1} - X_i)$, затем выделению каждого члена суммы в отдельный множитель, который ограничивается через лемму Хёффдинга. 

\subsection{Неравенство МакДиармида}

\begin{theorem}
  Пусть $X_1, \dots, X_n$ --- независимые с.в., определенные на $\Omega_1, \dots, \Omega_n$ соответственно. Пусть $\Omega = \Omega_1 \times \dots \times \Omega_n$. Пусть также есть функция $f: \Omega \to \R$, для которой существует такой набор чисел $c_1, \dots, c_n$, что для любых двух векторов элементарных исходов $\omega, \nu \in \Omega$, отличных только в $i$-ом компоненте верно, что $|f(\omega) - f(\nu)| \le c_i$. Пусть также $X = f(X_1, \dots, X_n)$. Тогда
  \begin{align*}
    \Pr[X - E[X] \ge \lambda] &\le \exp\left(-\frac{2\delta^2}{\sum_{i = 1}^{n - 1}c_i}\right), \\
    \Pr[X - E[X] \le -\lambda] &\le \exp\left(-\frac{2\delta^2}{\sum_{i = 1}^{n - 1}c_i}\right). \\
  \end{align*}
\end{theorem}

\end{document}