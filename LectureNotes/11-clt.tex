\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}
\pgfmathdeclarefunction{circle}{1}{%
\pgfmathparse{2/pi*sqrt(1 - #1^2)}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\eps{\varepsilon}
\newcommand\one{\mathbbm{1}}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\title{Лекция 11. Центральная предельная теорема, равенство Вальда.}

\begin{document}
\maketitle

\section{Центральная предельная теорема}

ЗБЧ давал нам только понять, что среднее первых $n$ с.в. из какой-то последовательности сходится к своему матожиданию (во всех смыслах сходимости). Однако, он ничего не говорил про то, какое распределение у суммы с.в. Рассмотрим пример. Пусть есть последовательность одинаково распределенных, независимых с.в. $X_n$, имеющих плотность вероятности $f_X(x) = \frac{2}{\pi}\sqrt{1 - x^2}$. У всех них матожидание $\mu = 0$ и дисперсия $\sigma^2 = \frac{1}{4}$ (легко посчитать).

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$f_{X}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    xmax=2,
    xmin=-2,
    ymax=1.1
    % legend pos = north east
    ]
      \addplot[domain=-1:1, samples=31, draw=blue, ultra thick]{circle(x)};
    \end{axis}
  \end{tikzpicture}
\end{center}

Пусть $M_n = \frac{1}{n} \sum_{i = 1}^n X_i$ --- среднее первых $n$ с.в. Из ЗБЧ мы знаем, что она с большой вероятностью несильно отклоняется от 1, то есть имеет примерно такую плотность вероятности:

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$f_{M_n}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    xmax=2,
    xmin=-2,
    % legend pos = north east
    ]
      \addplot[domain=-1:1, samples=31, draw=blue, ultra thick]{gauss(x, 0, 0.01)};
    \end{axis}
  \end{tikzpicture}
\end{center}

Можно легко посчитать матожиание $E[M_n] = 0$ и дисперсию $\Var(M_n) = \frac{\sigma^2}{n}$. Однако это мало о чем нам говорит, если мы хотим узнать что-то о распределении суммы этих величин. Пусть $S_n = \sum_{i = 1}^n X_i$. Тогда $E[S_n] = 0$, а дисперсия очень большая, $\Var(S_n) = n\sigma^n$.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$f_{S_n}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    ymin=0, ymax = 0.05
    % legend pos = north east
    ]
      \addplot[domain=-100:100, samples=61, draw=blue, ultra thick]{gauss(x, 0, 100)};
    \end{axis}
  \end{tikzpicture}
\end{center}

Однако что если мы рассмотрим другую нормировку, а именно $Z_n = \frac{S_n}{\sqrt{n}}$? Тогда дисперсия $Z_n$ будет такой же, как у любого $X_i$, а именно $\sigma^2$.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$f_{Z_n}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    xmin = -2, xmax = 2
    % legend pos = north east
    ]
      \addplot[domain=-2:2, samples=61, draw=blue, ultra thick]{gauss(x, 0, 0.25)};
    \end{axis}
  \end{tikzpicture}
\end{center}

И тут можно заметить, что распределение $Z_n$ подозрительно похоже на нормальное. В этом на самом деле и заключается центральная предельная теорема в ее классическом виде.

\begin{theorem}[Центральная предельная теорема]
  Пусть $\{X_n\}_{n \in \N}$ --- последовательность независимых одинаково распределенных с.в. с конечными матожиданиями $\mu$ и дисперсиями $\sigma^2$. Пусть $S_n = \sum_{i = 1}^n X_i$ и $Z_n = \frac{S_n - \mu n}{\sqrt{n} \sigma}$. Пусть также $Z$ --- это с.в., следующая стандартному нормальному распределению $N(0, 1)$. Тогда $Z_n$ сходится к $Z$ по распределению.
  \begin{center}
    \begin{tikzpicture}[rounded corners]
      \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
      \begin{minipage}{0.4\textwidth}
        \[
          \forall z \lim_{n \to \infty} \Pr(Z_n \le z) = \Pr(Z \le z)  
        \]
      \end{minipage}};
    \end{tikzpicture}
  \end{center}
\end{theorem}
Часто в эту классическую формулировку включают и то, что сходимость эта --- равномерная.

Мы опустим доказательство, так как оно основано на характеристических функциях и теореме Леви о непрерывности, а это немного выходит за рамки нашего курса. 

Существует несколько других вариантов ЦПТ, которые накладывают меньше требований на последовательность $X_n$. 


\subsection{Теоремы, не требующие одинакового распределения}
\begin{theorem}[Теорема Ляпунова]
  Пусть $\{X_n\}_{n \in \N}$ --- последовательность независимых с.в. с конечными матожиданиями и дисперсиями. Причем $E[X_n] = \mu_n$ и $\Var(X_n) = \sigma_n^2$. Пусть $S_n = \sum_{i = 1}^n X_i$, $s_n^1 = E[S_n]$ и $s_n^2 = \Var(S_n) = \sum_{i = 1}^n \sigma_i^2$. Пусть также для какого-то $\delta > 0$ выполнено условие Ляпунова:
  \begin{align*}
    \lim_{n \to +\infty} \frac{1}{s_n^{2 + \delta}} \sum_{i = 1}^n E\left[|X_i - \mu_i|^{2 + \delta}\right] = 0.
  \end{align*}
  Тогда
  \begin{align*}
    \frac{1}{s_n^1} \sum_{i = 1}^n (X_i - \mu_i) \to N(0, 1),
  \end{align*} 
  где сходимость --- по распределению.
\end{theorem}

\begin{theorem}[Теорема Линдеберга]
  Теорема Ляпунова также верна, если вместо условия Ляпунова для любого $\eps > 0$ выполнено
  \begin{align*}
    \lim_{n \to +\infty} \frac{1}{s_n^2} \sum_{i = 1}^n E\left[(X_i - \mu_i)^2 \one_{\{|X_i - \mu_i| > \eps s_n\}}\right] = 0.
  \end{align*}
\end{theorem}

Существуют также версии ЦПТ для последовательностей зависимых с.в. Например, когда будем проходить мартингалы -- пройдем ЦПТ для них.

\subsection{Практическое значение ЦПТ}

ЦПТ позволяет обращаться с $Z_n$ как со с.в., следующей стандартному нормальному распределению. Так как $S_n = \sqrt{n}\sigma Z_n + n\mu$, мы с ней можем обращаться как со с.в., следующей $N(n\mu, n\sigma^2)$ (но с меньшей точностью). Кстати про точность. Вообще ЦПТ ничего не говорит про скорость поточечной сходимости, но на практике обычно даже для небольших $n$ уже дает довольно хорошие приближения. На этот счет есть теорема Берри-Эссена (для случая независимых, одинаково распределенных с.в.), которая гласит, что если есть конечный тертий момент у всех с.в. последовательности (то есть $E[|X_n|^3] < \infty$), то сходимость к нормальному распределению имеет порядок как минимум $O(1/\sqrt{n})$. Более формально, она звучит так.

\begin{theorem}[Теорема Берри-Эссена]
  Пусть $\{X_n\}_{n \in \N}$ --- последовательность независимых одинаково распределенных с.в. с нулевыми матожиданиями, конечными дисперсиями $\sigma^2$ и конечными третими моментами $\rho = E[|X_n|^3]$. Пусть $S_n = \sum_{i = 1}^n X_i$ и $Z_n = \frac{S_n}{\sqrt{n} \sigma}$. Пусть также $F_n(x)$ --- функция распределения $Z_n$, а $\Phi(x)$ --- функция распределения стандартного нормального распределения $N(0, 1)$. Тогда существует такая константа $C$, такая что для всех $n$ и для всех $x$ верно, что
  \begin{align*}
    |F_n(x) - \Phi(x)| \le \frac{C\rho}{\sigma^3 \sqrt{n}}.
  \end{align*}
\end{theorem}

Из не очень проверенных мной источников известно, что $C$ может быть не очень большой, чуть больше $0.33$. Это все означает, что ЦПТ дает уже довольно хорошее приближение даже при не очень больших $n$. Рассмотрим примеры для некоторых дискретных с.в.

Пусть все $X_n \sim U(1, 8)$ (дисркетное равномерно распределение). Посмотрим на графики функции вероятности $S_n$ для разных $n$

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$p_{S_1}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    ymin = 0,
    title={$n = 1$}
    % legend pos = north east
    ]
      \addplot[draw=blue, only marks]
      coordinates{(1, 0.125)
      (2, 0.125)
      (3, 0.125)
      (4, 0.125)
      (5, 0.125)
      (6, 0.125)
      (7, 0.125)
      (8, 0.125)
      };
    \end{axis}
  \end{tikzpicture}
\end{center}

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$p_{S_2}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    ymin = 0,
    title={$n = 2$}
    % legend pos = north east
    ]
      \addplot[draw=blue, only marks]
      coordinates{(2, 0.015625)
      (3, 0.03125)
      (4, 0.046875)
      (5, 0.0625)
      (6, 0.078125)
      (7, 0.09375)
      (8, 0.109375)
      (9, 0.125)
      (10, 0.109375)
      (11, 0.09375)
      (12, 0.078125)
      (13, 0.0625)
      (14, 0.046875)
      (15, 0.03125)
      (16, 0.015625)
      };
    \end{axis}
  \end{tikzpicture}
\end{center}

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$p_{S_4}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    ymin = 0,
    title={$n = 4$}
    % legend pos = north east
    ]
      \addplot[draw=blue, only marks]
      coordinates{(4, 0.000244140625)
      (5, 0.0009765625)
      (6, 0.00244140625)
      (7, 0.0048828125)
      (8, 0.008544921875)
      (9, 0.013671875)
      (10, 0.0205078125)
      (11, 0.029296875)
      (12, 0.039306640625)
      (13, 0.0498046875)
      (14, 0.06005859375)
      (15, 0.0693359375)
      (16, 0.076904296875)
      (17, 0.08203125)
      (18, 0.083984375)
      (19, 0.08203125)
      (20, 0.076904296875)
      (21, 0.0693359375)
      (22, 0.06005859375)
      (23, 0.0498046875)
      (24, 0.039306640625)
      (25, 0.029296875)
      (26, 0.0205078125)
      (27, 0.013671875)
      (28, 0.008544921875)
      (29, 0.0048828125)
      (30, 0.00244140625)
      (31, 0.0009765625)
      (32, 0.000244140625)      
      };
    \end{axis}
  \end{tikzpicture}
\end{center}

То есть уже при $n = 4$ распределение уже очень похоже на нормальное. Однако если у нас нет такой хорошей симметрии в распределнии $X_n$, то все не так гладко. Пусть $X_n \sim \Geom(1/2)$. Посмотрим на картинки в этом случае.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$p_{X}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    ymin = 0,
    title={$n = 1$}
    % legend pos = north east
    ]
      \addplot[draw=blue, only marks]
      coordinates{(1, 0.5)
      (2, 0.25)
      (3, 0.125)
      (4, 0.0625)
      (5, 0.03125)
      (6, 0.015625)
      (7, 0.0078125)
      (8, 0.00390625)
      };
    \end{axis}
  \end{tikzpicture}
\end{center}

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
    grid=major,
    ylabel={$p_{S_{32}}(x)$},
    xlabel={$x$},
    width = 0.6\textwidth,
    height = 5cm,
    ymin = 0,
    title={$n = 32$}
    % legend pos = north east
    ]
      \addplot[draw=blue, only marks]
      coordinates{
      (32, 7.450580596923828e-09)
(33, 6.146728992462158e-08)
(34, 3.4831464290618896e-07)
(35, 1.5238765627145767e-06)
(36, 5.485955625772476e-06)
(37, 1.69150298461318e-05)
(38, 4.591222386807203e-05)
(39, 0.00011191104567842558)
(40, 0.0002486912126187235)
(41, 0.0005098169858683832)
(42, 0.0009732869730214588)
(43, 0.001743805826663447)
(44, 0.0029510560143535263)
(45, 0.004742768594496738)
(46, 0.0072722451782283315)
(47, 0.010681110105522862)
(48, 0.015079214266620513)
(49, 0.020524486085122366)
(50, 0.02700590274358206)
(51, 0.03443252599806712)
(52, 0.04263074647379739)
(53, 0.0513506718888923)
(54, 0.06028122352174314)
(55, 0.0690722352853307)
(56, 0.07736090351957037)
(57, 0.08479945193491364)
(58, 0.09108089281898135)
(59, 0.09596022636285535)
(60, 0.09926919968571245)
(61, 0.10092368634714095)
(62, 0.10092368634714097)
(63, 0.09934675374796685)
(64, 0.09633624605863457)
(65, 0.09208611755604772)
(66, 0.08682405369570213)
(67, 0.08079460552238951)
(68, 0.07424369156111467)
(69, 0.06740545681206464)
(70, 0.06049207662621188)
(71, 0.053686718005763016)
(72, 0.047139557273352894)
(73, 0.040966520011366206)
(74, 0.035250261405129074)
(75, 0.03004283642482591)
(76, 0.025369506314297432)
(77, 0.021233173763053285)
(78, 0.0176190165267889)
(79, 0.01449898235017003)
(80, 0.011835903959322472)
(81, 0.009587082207051203)
(82, 0.007707262166452928)
(83, 0.006150988075149933)
(84, 0.004874367908609383)
(85, 0.0038363080762203474)
(86, 0.002999295405044997)
(87, 0.0023298098235617387)
(88, 0.0017984496883634482)
(89, 0.0013798450195202318)
(90, 0.0010524241674306854)
(91, 0.0007980883269682694)
(92, 0.0006018370990252524)
(93, 0.00045137782426893944)
(94, 0.0003367421863593675)
(95, 0.0002499258414385931)
(96, 0.00018456062137003792)
(97, 0.00013562409297646722)

      };
    \end{axis}
  \end{tikzpicture}
\end{center}

Тут уже даже при $n = 32$ все выглядит не так симметрично (хотя и довольно похоже на нормальное распределение).

\subsection{Как пользоваться ЦПТ}

ЦПТ позволяет приближать сумму с.в. нормальным распределением, а значит для оценок вероятностей мы можем пользоваться таблицами для нормального распределения. Например, рассмотрим такую задачу. На корабль грузят контейнеры, вес каждого контейнера -- с.в., следующая экспоненциальному распределению с параметром $\lambda = 1/2$ (то есть $\mu = \sigma = \frac{1}{\lambda} = 2$).  Загрузили 100 контейнеров. Какова вероятность, что суммарный вес превзойдет 210?

По сути мы хотим вычислить $\Pr(S_{100} \ge 210)$. Но $S_n$ не приближается стандартным нормальным распределением, а $Z_n = \frac{S_n - n\mu}{\sqrt{n}\sigma}$ --- приближается. Поэтому
\begin{align*}
  \Pr[S_{100} > 210] = \Pr\left[\frac{S_{100} - 100 \cdot 2}{\sqrt{100} \cdot 2} \ge \frac{210 - 100 \cdot 2}{\sqrt{100} \cdot 2}\right] = \Pr[Z_n \ge 0.5] = 1 - \Phi(0.5) \approx 0.3085.
\end{align*}

\textbf{Дискретный случай}

Посмотрим, насколько хороршо работает ЦПТ в случае дискретных распределений. Пусть $\{X_n\}_{n \in \N}$ --- последовательность с.в. Бернулли с параметром $p = \frac{1}{2}$ (то есть $\mu = \frac{1}{2}$, $\sigma = \sqrt{p(1 - p)} = \frac{1}{2}$). Тогда $S_n \sim \Bin(n, \frac{1}{2})$ и $Z_n = \frac{S_n - n/2}{\sqrt{n}/2}$ примерно следует стандартному нормальному распределению. Но насколько точно? Посчитаем на примере. Пусть $n = 36$. Посчитаем $\Pr(S_n \le 21)$ точно:

\begin{align*}
  \Pr(S_n \le 21) = \sum_{i = 0}^n \binom{36}{i} 2^{-36} \approx 0.8785
\end{align*}

Попробуем применить ЦПТ:

\begin{align*}
  \Pr[S_n \le 21] = \Pr\left[Z_n \le \frac{21 - 18}{3}\right] = \Pr[Z_n \le 1] \approx 0.8413.
\end{align*}

Получилось довольно большая погрешность. Попробуем посчитать по-другому. В дискретном случае $\Pr[S_n \le 21] = \Pr[S_n < 22]$, поэтому

\begin{align*}
  \Pr[S_n \le 21] = \Pr\left[Z_n < \frac{22 - 18}{3}\right] = \Pr[Z_n < 1.33] \approx 0.9082.
\end{align*}

Опять погрешность довольно велика. Поэтому часто для дискретных с.в. пользуются правилом коррекци на $1/2$, а именно
\begin{align*}
  \Pr[S_n \le 21] = \Pr[S_n \le 21.5]\Pr\left[Z_n < \frac{21.5 - 18}{3}\right] = \Pr[Z_n < 1.165] \approx 0.8780.
\end{align*}
И это уже получается довольно точно.

С такой коррекцией можно достаточно точно аппроксимировать даже функцию вероятностей биномиального распеределения (рассмотрим другое число для того, чтобы не зацикливаться на 21)

\begin{align*}
  \Pr[S_n = 19] = \Pr[18.5 \le S_n \le 19.5] \approx \Pr[0.166 \le Z_n \le 0.5] = \Phi(0.5) - \Phi(0.17) \approx 0.124
\end{align*}
Если же посчитать эту вероятность точно, то $\Pr[S_n = 19] = \binom{36}{19} 2^{-36} \approx 0.1251$. На этот счет есть теорема Муавра-Лапласа, но работает она только для значений функции вероятностей в районе $E[S_n] = np$

\begin{theorem}{Теорема Муавра-Лапласа}
  Для любых $p \in [0, 1]$ и $q = 1 - p$ если $\lim_{n \to +\infty} \frac{k}{n} = p$, то   
  \begin{align*}
    \lim_{n \to +\infty} \frac{\binom{n}{k}p^kq^{n - k}}{\frac{1}{\sqrt{2\pi npq}} e^{-\frac{(k - np)^2}{2npq}}} = 1.
  \end{align*}
\end{theorem}

В оригинале доказывается через формулу Стирлинга, но это очень громоздко, и мы этого делать не будем.

\section{Равенство Вальда}

У нас уже была упрощенная версия этой теоремы, однако давайте рассмотрим обобщенный случай.

\begin{theorem}[Равенство Вальда]
  Пусть $\{X_n\}_{n \in \N}$ --- бесконечная последовательность вещественных с.в. Пусть также $N$ --- какая-то целочисленная неотрицательная с.в. (то есть $N$ всегда принимает значения из $\N_0$). Допустим также, что выполнены все условия:
  \begin{enumerate}
    \item Все $X_n$ имеют конечное матожидание.
    \item Для любого $n \in \N$ верно, что $E[X_n \one_{\{N \ge n\}}] = E[X_n] \Pr(N \ge n)$ (можно трактовать как событие $N \ge n$ не очень зависит от с.в. $X_n$).
    \item Следующий ряд сходится (трактовка: просто техническое требование для абсолютной сходимости $E[S_N]$ и $E[T_N]$, но также часто встречающееся на практике, особенно когда $X_n$ неотрицательные).
    \begin{align*}
      \sum_{n = 1}^{+\infty} E[|X_n|\one_{\{N \ge n\}}] < +\infty
    \end{align*}
  \end{enumerate}
  Тогда, если мы обозначим 
  \begin{align*}
    S_N &= \sum_{n = 1}^N X_n, \\
    T_N &= \sum_{n = 1}^N E[X_n],
  \end{align*}
  то верно, что $E[S_N] = E[T_N]$
  
  Если также $N$ имеет конечное матожидание и все $X_n$ имеют одинаковое конечное матожидание, то
  \begin{align*}
    E[S_N] = E[X_1] E[N].
  \end{align*}
\end{theorem}

Именно последнее равенство часто и имеют в виду, когда говорят о равенстве Вальда. Заметим, что по сравнению с тем, что у нас было раньше, мы не требуем полной независимости $X_n$ от $N$, а также допускаем, что у $X_n$ могут быть разные распределения.

Заметим, что в общем случае все три условия необходимы. Если не выполняется первое, то суммы $S_N$ и $T_N$ просто неопределены. 

Необходимость второго условия демонстрируется следующим примером. Возьмем последовательность $X_n$, которые следуют распределению Бернулли с $p = 0.5$. И возьмем довольно зависимый от них $N = 1 - X_1$. В таком случае $S_N$ всегда равно нуль (либо это сумма из нуля слагаемых, либо это одно слагаемое, равное нулю), однако $E[X_1]E[N] = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}$. В данном примере есть явная зависимость события $N \ge 1$ от $X_1$, причем 

\begin{align*}
  E[X_1 \one_{\{N \ge 1\}}] = \frac{1}{2} \cdot 1 \cdot 0 + \frac{1}{2} \cdot 0 \cdot 1 = 0 \ne \frac{1}{2} \cdot \frac{1}{2} = E[X_1] \Pr[N \ge 1].
\end{align*}

Для необходимости третьего (более технического) условия рассмотрим последовательность $X_n = \pm 2^n$, причем знак выбирается равновероятно. И возьмем $N = \min_n \{X_n = +2^n\}$, то есть первый раз, когда мы взяли положительную степень двойки. В такос случае выполняется первое условие, $E[X_n] = 0$, выполняется и второе, так как событие $N \ge n$ зависит только от $X_1, \dots, X_{n - 1}$, но не от $X_n$. При этом третье условие не выполнено:
\begin{align*}
  E[|X_n|\one_{\{N \ge n\}}] = 2^n \cdot \left(\frac{1}{2}\right)^{n - 1} = \frac{1}{2}.
\end{align*}

В таком случае $S_N = 2$, так как независимо от $N$ эта сумма равна $2^N - \sum_{n = 1}^{N - 1} 2^n = 2$. Но так как $E[X_n] = 0$, то справа в равенстве ноль.

Также пара примеров, когда все равботает. Очевидный пример --- задача про сумму очков, которая выпадает на кости до первой выпавшей единицы. Мы раньше считали матожидание этой суммы так:
\begin{align*}
  \sum_{i = 1}^{+\infty} \frac{1}{6} \left(\frac{5}{6}\right)^{i - 1} (4 (i - 1) + 1) = 21
\end{align*}
По равенству Вальда получается то же самое (только надо проверить удовлетворение всех условий)
\begin{align*}
  E[S_N] = E[N] E[X_1] = 6 \cdot 3.5 = 21
\end{align*}

Также неравенство Вальда работает при зависимых друг от друга $X_n$. Пусть есть какая-то с.в. $Z$ с нулевым матожиданием, и мы определяем $X_n = (-1)^n Z$. По равенству Вальда матожидание их суммы равно нулю. Если рассуждать по-другому, то если $N$ четное, то сумма равна нулю, а если нечетное, то $E[S_N] = E[-Z] = 0$. Но опять же надо проверить все условия.

\textbf{Доказательство равенства Вальда}

Сначала надо доказать абсолютную сходимость $E[S_N]$ и $E[T_N]$.

\begin{align*}
  E[|S_N|] &= E\left[\sum_{i = 1}^{+\infty}|S_i|\one_{N = i}\right] \le E\left[\sum_{i = 1}^{+\infty}\sum_{j = 1}^i |X_j| \one_{N = i}\right] \\
  &= E\left[\sum_{j = 1}^{+\infty} |X_j| \sum_{i = j}^{+\infty} \one_{N = i}\right] = E\left[\sum_{j = 1}^{+\infty} |X_j| \one_{N \ge j}\right] \\
  &= \sum_{j = 1}^{+\infty} E\left[|X_j| \one_{N \ge j}\right] < \infty,
\end{align*}
где переход со второй на третью строчку легален из-за конечности финального ряда, а последнее неравенство следует из условий теоремы.

\begin{align*}
  E[|T_N|] &= \sum_{i = 1}^{+\infty} |T_i| \Pr[N = i] \le \sum_{i = 1}^{+\infty} \Pr[N = i] \sum_{j = 1}^{i} |E[X_j]| \\
  &= \sum_{j = 1}^{+\infty} |E[X_j]| \Pr[N \ge j]  = \sum_{j = 1}^{+\infty} |E[X_j \one_{N \ge j}]| \\
  &\le \sum_{j = 1}^{+\infty} E[|X_j| \one_{N \ge j}] < \infty,
\end{align*}
где в середине второй строчки мы воспользовались вторым условием равенства, а в конце -- третьим. Также мы дважды воспользовались неравенством треугольника (в обоих нестрогих неравенствах). Теперь легально доказать само равенство следующим образом:

\begin{align*}
  E[S_N] &= E\left[\sum_{i = 1}^{+\infty} X_i \one_{N \ge i}\right] = \sum_{i = 1}^{+\infty} E\left[X_i \one_{N \ge i}\right] \\
  &= \sum_{i = 1}^{+\infty} E[X_i] \Pr[N \ge i] = \sum_{i = 1}^{+\infty} E[X_i] \sum_{j = i}^{+\infty} \Pr[N = j] \\
  &= \sum_{j = 1}^{+\infty} \Pr[N = j] \sum_{i = 1}^j E[X_i] = \sum_{j = 1}^{+\infty} \Pr[N = j] T_j = E[T_N]. 
\end{align*}

Мы везде могли менять местами порядок суммирования, так как все у нас сходится абсолютно, плюс воспользовались вторым условием при переходе на вторую строчку. Наконец, если у всех $X_n$ одинаковое матожидание, и у $N$ конечное матожидание, то
\begin{align*}
  E[S_N] = E[N E[X_1]] = E[N] E[X_1],
\end{align*}

Так как $E[X_1]$ в матожидании расценивается как константа.

\end{document}
