\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}
\pgfmathdeclarefunction{circle}{1}{%
\pgfmathparse{2/pi*sqrt(1 - #1^2)}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\F{\mathcal{F}}
\newcommand\eps{\varepsilon}
\newcommand\one{\mathbbm{1}}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Pois}{Poisson}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\title{Лекция 14. Процесс Пуассона}

\begin{document}
\maketitle

\section{Аппроксимация процесса Бернулли}

Рассмотрим процесс Бернулли с параметром $p$. Чему равна вероятность того, что за время $n$ будет $k$ успехов? По тому, что мы считали, это будет 
\begin{align*}
  p_X(k) = \binom{n}{k} p^k (1 - p)^{n - k}.
\end{align*}

Но что будет, если мы раздробим время на меньшие интервалы (при этом вероятность успеха в каждый интервал будет падать обратно пропорционально длине интервала)? То есть для каждого $n$ мы будем брать $p = {\lambda}{n}$ для какой-то фиксированной $\lambda$ и устремим $n \to \infty$. В таком случае будет:

\begin{align*}
  p_X(k) &= \binom{n}{k} p^k (1 - p)^{n - k} \\
         &= \frac{n(n - 1)\dots(n - k+ 1)}{k!} \cdot \frac{\lambda^k}{n^k} \left(1 - \frac{\lambda}{n}\right)^{n - k} \\
         &= \frac{n}{n} \cdot \frac{n - 1}{n} \cdot \ldots \cdot \frac{n - k + 1}{n} \cdot \frac{\lambda^k}{k!} \left(1 - \frac{\lambda}{n}\right)^n \left(1 - \frac{\lambda}{n}\right)^{-k} \\
         &\to 1 \cdot 1 \cdot \ldots \cdot 1 \cdot \frac{\lambda^k}{k!}e^{-\lambda} \cdot 1 = \frac{\lambda^k}{k!}e^{-\lambda}.
\end{align*}

И это подводит нас к процессу Пуассона.

\section{Процесс Пуассона: определение}

Процессом Пуассона называется некоторый процесс с вещественным временем, и в каждый момент времени происходит или не происходит событие. Однако в силу непрерывности вероятность, что в каждый конкретный момент времени произойдет событие равна нулю, зато на временном интервале может произойти более одного события. Для того, чтобы такой процесс назывался процессом Пуассона, мы требуем:
\begin{enumerate}
  \item Независимость непересекающихся временных интервалов: число событий, произошедших в интервал времени $\tau_1$ не зависит от числа событий в другом интервале $\tau_2$.
  \item Неизменяемость по времени. Вероятность $P_\tau(k)$ того, что в интервал длиной $\tau$ произойдет $k$ событий не зависит от того, в какой именно момент времени начался интервал. Заметим, что для любого $\tau$ выполняется $\sum_{k = 0}^{+\infty} P_\tau(k) = 1$.
  \item Для маленьких интервалов длиною $\delta$ мы также требуем
  \begin{align*}
    P_\delta(k) \approx \begin{cases}
      1 - \lambda\delta, &k = 0 \\
      \lambda\delta, &k = 1 \\
      0, &\text{ иначе.}
    \end{cases}
  \end{align*}  
  То есть мы хотим, чтобы для достаточно маленького интервала вероятность того, что в нем уместилось более двух событий была ничтожно маленькая, а вероятность одного события была пропорционально длине интервала. Более точное требование: при $\delta \to 0$ должно выполняться
  \begin{align*}
    P_\delta(k) = \begin{cases}
      1 - \lambda\delta + O(\delta^2), &k = 0 \\
      \lambda\delta + O(\delta^2), &k = 1 \\
      O(\delta^2), &\text{ иначе.}
    \end{cases}
  \end{align*}
  В данном случае $\lambda$ называется \emph{интенсивностью} процесса.
\end{enumerate}

Примеры из реальной жизни:
\begin{enumerate}
  \item Смерти от удара лошади в Прусской армии (самое первое применение, причем не у Пуассона, а позже Борткевичем)
  \item Испускание радиоактивной частицы
  \item Детекция фотона от слабого источника
  \item Потрясения рынка (финансовые кризисы и т.п.)
  \item Сфера обслуживания: звонки в колл-центры, приход покупателя, ...
\end{enumerate}

\section{Распределение Пуассона}
Пусть случайная величина $N_\tau$ --- число событий в процессе Пуассона с интенсивностью $\lambda$, которые произошли в интервал длиной $\tau$. Попробуем оценить ее распределение. Для этого разобьем весь интервал длиной $\tau$ на отрезки (слоты) длиной $\delta$, всего получится $\frac\tau\delta$ отрезков. При этом вероятность $p$ того, что есть хотя бы один отрезок, куда попали два или более событий не больше, чем сумма вероятностей по всем отрезкам того, что в этот отрезок попало два и более событий. По третьему свойству процесса Пуассона имеем.
\begin{align*}
  p \le \sum_{i = 1}^{\tau/\delta} \Pr[N_\delta \ge 2] = \frac{\tau}{\delta} O(\delta^2) = O(\tau \delta).
\end{align*}

Это значит, что мы можем добиться сколь угодно маленькой вероятности того, что у нас есть слот с двумя и более событиями, а значит, мы можем считать, что $N_\tau \sim \Bin(\frac{\tau}{\delta}, \lambda\delta)$. При этом в начале лекции мы показали, что
\begin{align*}
  \Pr[N_\tau = k] = \lim_{\delta \to 0} \Pr[X \sim \Bin(\frac{\tau}{\delta}, \lambda\delta + O(\delta^2)) = k] = \frac{(\lambda\tau)^k}{k!}e^{-\lambda\tau}.
\end{align*}
Говорят, что $N_\tau$ в таком случае следует распределению Пуассона с параметром $(\lambda\tau)$:
\begin{align*}
  N_\tau \sim \Pois(\lambda\tau).
\end{align*}

\section{Свойства распределения Пуассона}
\textbf{Матожидание и дисперсия.}

Матожидание распределения Пуассона можно посчитать напрямую
\begin{align*}
  E[N_\tau] &= \sum_{k = 0}^{+\infty} k \frac{(\lambda\tau)^k}{k!}e^{-\lambda\tau} \\
            &= e^{-\lambda\tau} \lambda\tau \sum_{k = 0}^{+\infty} \frac{(\lambda\tau)^k}{(k - 1)!}  \\
            &= \lambda\tau.
\end{align*}
А можно сказать, что оно близко к биномиальному распределению $\Bin(\frac{\tau}{\delta}, \lambda\delta + O(\delta^2))$, то есть его матожидание есть
\begin{align*}
  E[N_\tau] = \lim_{\delta \to 0} \frac{\tau}{\delta} (\lambda\delta + O(\delta^2)) = \lambda\tau.
\end{align*}
Дисперсию тоже посчитаем этим методом:
\begin{align*}
  \Var(N_\tau) = \lim_{\delta \to 0} \frac{\tau}{\delta} (\lambda\delta + O(\delta^2)) (1 - \lambda\delta + O(\delta^2)) = \lambda\tau.
\end{align*}
То есть для распределения Пуассона с параметром $\lambda\tau$
\begin{align*}
  E[N_\tau] = \Var(N_\tau) = \lambda\tau
\end{align*}
Интенсивность процесса Пуассона при этом можно интерпретировать как среднее число событий, происходящих в единицу времени:
\begin{align*}
  \lambda = \frac{E[N_\tau]}{\tau}.
\end{align*}

\textbf{Время первого события $T_1$.}

Вероятность того, что первое событие произошло через время большее, чем $t$, есть вероятность того, что за время $t$ произошло ноль событий, поэтому можем посчитать функцию распределения $T_1$.
\begin{align*}
  F_{T_1}(t) = \Pr[T_1 \le t] = 1 - \Pr[T_1 > t] = 1 - P_t(0) = 1 - e^{-t\lambda}.
\end{align*}
Получается, время $T_1$ следует распределению $\Exp(\lambda)$.
Заметим, что в силу беспамятства экспоненциального распределения, при условии $T_1 > t$ величина $T_1 - t$ также следует $\Exp(\lambda)$.

\textbf{Время $k$-ого события $T_k$.}

Можно попытаться посчитать функцию распределения $T_k$ тем же образом:
\begin{align*}
  \Pr[T_k \le t] = \sum_{i = k}^{+\infty} P_t(i),
\end{align*}
но это черевато дебрями сложной математики (остаток ряда экспоненты выглядит не очень мило). Поэтому давайте попробуем посчитать плотность вероятности следующим образом:
\begin{align*}
  f_{T_k}(t) \delta \approx \Pr[t \le T_k \le t + \delta] = P_t(k - 1) (\lambda \delta + O(delta^2)) + (\sum_{i = 0}^{k - 2} P_t(i)) O(\delta(2)) \approx  P_t(k - 1)\lambda \delta.  
\end{align*}
Таким образом, 
\begin{align*}
  f_{T_k}(t) \approx \lambda P_t(k - 1) = \frac{\lambda^k t^{k - 1}}{(k - 1)!}e^{-(k - 1)t}.
\end{align*}
Это, кстати, называется распределением Эрланга (частный случай Гамма-распределения, как некоторые из вас знают с контрольной).

\textbf{Беспамятство.}

Как и в случае с процессом Бернулли, у процесса Пуассона есть беспамятство. Рассмотрим случай, когда мы начинаем смотреть процесс в фиксированное время $t$. Тогда для наблюдаемого нами процесса по-прежнему выполняются все три свойства процесса Пуассона. То же верно, когда $T$ --- время останова.

Поэтому если мы начинаем смотреть в любой фиксированный момент времени, то время до следующего события следует экспоненциальному распределению. Также если мы начинаем смотреть в момент $T_k$, когда произошло событие $k$, то время до следующего события $Y_{k + 1} = T_{k + 1} - T_k$ также следует $\Exp(\lambda)$. Поэтому существует альтернативное определение процесса Пуассона.

Пусть есть последовательность независимых с.в. $\{Y_k\}_{k \in \N}$, следующих распределению $\Exp(\lambda)$. Тогда последовательность с.в. $T_k = \sum_{i = 1}^k Y_k$ образует процесс Пуассона.

Отсюда следуют свойства $T_k$:
\begin{itemize}
  \item $E[T_k] = \frac{k}{\lambda}$
  \item $\Var(T_k) = \frac{k}{\lambda^2}$
\end{itemize}

\section{Сравнение с процессом Бернулли}
\begin{center}
  \begin{tabular}{p{5cm}|c|c}
    & Пуассон & Бернулли \\\hline
    Время & Непрерывное & Дискретное \\
    Интенсивность & $\lambda$ в единицу времени & $p$ в тайм-слот \\
    Распределение числа событий в интервале & $\Pois(\lambda\tau)$ & $\Bin(n, p)$ \\
    Интервалы между событиями & $\Exp(\lambda)$ & $\Geom(p)$ \\
    $k$-е событие & Эрланг & Паскаль (inverse $\Bin$) \\
  \end{tabular}
\end{center}

\section{Сумма двух с.в. Пуассона}

Рассмотрим процесс Пуассона с $\lambda = 1$. Рассмотрим два последовательных временных интервала длиной $\mu$ и $\nu$. Пусть $X$ --- число событий в первом интервале, а $Y$ --- число событий во втором. Тогда $X \sim \Pois(\mu)$ и $Y \sim \Pois(\nu)$, причем они независимы.

С.в. $Z = X+Y$ есть число событий в интервале длиной $\mu + \nu$, то есть $Z \sim \Pois(\mu + \nu)$. Таким образом, для двух независимых с.в. Пуассона выполняется
\begin{align*}
  \Pois(\mu) + \Pois(\nu) = \Pois(\mu + \nu),
\end{align*}
что есть равенство распределений (то есть совпадение функции распределения во всех точках).

\section{Слияние двух процессов Пуассона}
Аналогично с процессами Бернулли, мы можем сливать процесс Пуассона из двух независимых процессов. Пусть у нас есть две лампочки разных цветов, зеленая и красная. Они независимо мигают с интенсивностью $\lambda_1$ и $\lambda_2$ (то есть мигания каждой лампочки --- процессы Пуассона). У нас есть датчик, регистрирующий вспышку, но не воспринимающий цвет. Тогда регистрации на датчике тоже будут процессом Пуассона.
\begin{enumerate}
  \item Выполняется независимость непересекающихся интервалов
  \item Выполняется неизменность во времени
  \item Рассмотрим вероятность того, то в интервале длины $\delta$ произошло $k$ событий
  \begin{center}
    \begin{tabular}{lr|ccc}
      & & $1 - \lambda_1 \delta$ & $\lambda_1\delta$ & $O(\delta^2)$ \\
      & & 0 & 1 & $\ge 2$ \\ \hline
      $1 - \lambda_2 \delta$ & 0  & $1 - (\lambda_1 + \lambda_2) \delta$ & $\lambda_1\delta$ & \\
      $\lambda_2 \delta$ & 1      & $\lambda_2\delta$ & & \\
      $O(\delta^2)$ & $\ge 2$     & & & $O(\delta^2)$ \\
    \end{tabular}
  \end{center}
  То есть интенсивность нового процесса есть $\lambda_1 + \lambda_2$.
\end{enumerate}

Также мы можем показать вероятность того, из какого процесса произошло $k$-е событие. Возьмем очень маленький интервал $\delta$, в котором это событие произошло, и по формуле условной вероятности и по таблице выше почитаем, что вероятность того, что событие пришло из первого процесса есть $\frac{\lambda_1}{\lambda_1 + \lambda_2}$.

Причем источники различных событий в слитом процессе независимы. То есть, последовательность источников событий образует процесс Бернулли.

\textbf{Пример.}

Пусть у нас есть три лампочки, время работы которых следует $\Exp(\lambda)$. Когда перегорит первая лампочка и когда перегорит последняя?

Насчет распределения времени перегорания первой можно посчитать в тупую тройной интеграл функции $\min\{T_1, T_2, T_3\}$, но это скучно (можно написать этот страшный интеграл на доске).

Можно посчитать через функцию распределения:
\begin{align*}
  \Pr[\min\{T_1, T_2, T_3\} \ge t] = \Pr[T_1 \ge t] \Pr[T_2 \ge t] \Pr[T_3 \ge t] = e^{-3 \lambda t}
\end{align*}

А можно просто сказать, что перегорание каждой лампочки --- это первое событие в процессе Пуассона с интенсивностью $\lambda$. Поэтому слитый процесс имеет интенсивность $3\lambda$, и первая лампочка перегорает через время $T \sim \Exp(3\lambda)$.

Для оценки того, когда перегорает последняя, можем заметить, что если мы смотрим на процесс после с момента перегорания первой лампочки, то мы смотрим на слияние двух процессов, которое имеет интенсивность $2\lambda$, то есть время между первой и второй следует $\Exp(2\lambda)$. Ну а последняя перегорает еще через $\Exp(\lambda)$. Таким образом, время перегорания последней лампочки следует распределению $\Exp(3\lambda) + \Exp(2\lambda) + \Exp(\lambda)$, где все три распределения --- независимы. 


\section{Разделение процесса Пуассона}

Рассмотрим процесс Пуассона с интенсивностью $\lambda$. Можем при каждом событии подкидывать нечестную монетку и с вероятностью $q$ отправлять событие в поток $A$ и с вероятностью $1 - q$ --- в поток $B$. Каждый из потоков будет процессом Пуассона, причем поток $A$ --- с интенсивностью $\lambda q$, а поток $B$ --- с интенсивностью $\lambda (1 - q)$. Будут ли они независимы? В отличие от процессов Бернулли --- да, будут, так как мы находимся в ситуации непрерывного времени.

\section{Парадокс наблюдателя}

Допустим, вы знаете, что автобусы ходят с интенсивностью 4 автобуса в час. То есть интервал в среднем должен быть 15 минут. Вы приходите на остановку, интересуетесь у продавца в ларьке, как давно ушел последний автобус и ждете следующего. Таким образом вы замеряете интервал между автобусами и уходите. Так вы делаете много дней подряд и обнаруживаете, что средний интервал 30 минут, а не 15. Почему так?

Когда мы пришли на остановку --- матожидание времени до следующего автобуса есть 15 минут из-за беспамятства процесса Пуассона. Однако ожидаемое время до предыдущего автобуса --- тоже 15 минут, так как мы можем рассматривать автобусы в прошлом как независимый от будущего процесс Пуассона, в котором время течет обратно. В чем подвох?

Подсказка. Пусть есть процесс, в котором события происходят через какие-то интервалы. Но при этом каждый интервал либо 10, либо 5 минут, причем равновероятно. То есть среднее время между событиями есть $7.5$ минут. Однако если мы хотим прийти в случайный момент времени и замерить длину интервала, в который мы пришли, то получится, что мы с вероятностью $\frac{2}{3}$ придем в 10-минутный интервал, так как они занимают треть от всей числовой оси. То есть для нас средняя длина отрезка будет $\frac{5}{3} + {20}{3} \approx 8.3$.

Также и с процессом Пуассона: мы намного вероятнее приходим на остановку в длинные интервалы.

Тут также стоит сказать, что подобный эффект может влиять на опросы.
\begin{itemize}
  \item Если мы опрашиваем случайных людей о размере их семьи, то наш средний результат будет выше, чем реальный размер семьи.
  \item Если мы опрашиваем случайных людей о заполненности автобуса, на котором они сегодня ехали --- средний ответ тоже будет выше среднего значения.
\end{itemize}

\end{document}