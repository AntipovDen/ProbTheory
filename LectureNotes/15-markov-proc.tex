\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta,snakes}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}
\pgfmathdeclarefunction{circle}{1}{%
\pgfmathparse{2/pi*sqrt(1 - #1^2)}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\F{\mathcal{F}}
\newcommand\eps{\varepsilon}
\newcommand\one{\mathbbm{1}}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Pois}{Poisson}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\title{Лекция 15. Марковские процессы}

\begin{document}
\maketitle

\section{Определение марковского процесса}

Мы уже рассмотрели процессы, в которых будущее никак не зависит от прошлого. Однако в реальной жизни часто бывает так, что будущее таки зависит от того, что было в прошлом или просто от настоящего. Во втором случае (когда зависимость только от настоящего), удобно ввести понятие \emph{состояния}, описывающее \emph{настоящее}. В случае с дискретным временем мы можем описать процесс как последовательность состояний $X_t$, где $X_{t + 1} = f(X_t, Y_t)$, где $f$ -- некоторая функция от предыдущего состояния и некоторого случайного шума $Y_t$. 

Рассмотрим пример очереди в супермаркете. Разобъем время на достаточно маленькие интервалы времени и в начале каждого интервала $t$ будем считать с.в. $X_t$, равную числу человек в очереди. В данном примере есть явная зависимость будущего от настоящего. Если $X_t = 0$, то вероятность того, что в начале следующего интервала очередь тоже будет пустой, а если $X_t = 10$, то эта вероятность мала. Причем при оценке этой вероятности совсем неважно, сколько было человек в очереди до времени $t$, то есть $X_{t - s}$ никак не влияет на $X_{t + 1}$.

Более конкретно, Марковским процессом называется процесс $\{X_t\}_{t \in \N}$, для которого одновременно выполняется:
\begin{enumerate}
  \item Для всех $t, \delta, h, i$ и $j$ выполняется \[\Pr[X_{t + \delta} = j \mid X_t = i] = \Pr[X_{t + h + \delta} = j \mid X_{t + h} = i],\] или \[f_{X_{t + \delta} \mid X_t}(j \mid i) = f_{X_{t + h + \delta} \mid X_{t + h}}(j \mid i).\] 
  Это свойство называется \emph{однородностью} во времени, то есть если мы оказались в состоянии $S_1$ в момент времени $t$, то вероятность оказаться в состоянии $S_2$ через время $\delta$ не зависит от момента времени $t$, а только от состояния в тот момент.
  \item Для всех $t, \delta, i$ и $j$, а также любых наборов $\{i_\tau\}_{\tau < t}$ выполняется \[\Pr[X_{t + \delta} = j \mid X_t = i, \cap_{\tau < t} X_\tau = i_\tau] = \Pr[X_{t + \delta} = j \mid X_t = i], \] или, для несчетного множества состояний, аналогичное равенство плотностей вероятностей (которое заморочно записывать тут в техе).
  Это называется \emph{марковское свойство}. Оно означает, что в момент времени $t$ все будущее процесса зависит только от текущего состояния, но не от остальной истории процесса. Иногда это записывается через стандартную фильтрацию:
  \[\Pr[X_{t + \delta} = j \mid \F_t] = \Pr[X_{t + \delta} = j \mid X_t]. \]
  Заметим, что это равенство случайных величин.
  \item Удовлетворение совйства вероятностной меры. Сумма или инеграл всех вероятностей перехода по всем возможным состояниям должна равняться единице для любого фиксированного $i$ и $\delta$.
  \[ \sum_j \Pr[X_{t + \delta} = j \mid X_t = i] = 1,\]
  или
  \[ \int_\R f_{X_{t + \delta} \mid X_t}(x \mid i) dx = 1. \]
\end{enumerate}

Марковсике процессы могут быть многих разных типов. Например,
\begin{itemize}
  \item Процессы с дискретным временем и процессы с непрерывным временем.
  \item Процессы с конечным, счетным или несчетным множестовм состояний.
\end{itemize}

К сожалению, данный курс не позволяет охватить весь спектр процессов, поэтому мы ограничимся только марковскими процессами \emph{с дискретным временем} и \emph{с конечным числом состояний}. Везде далее под марковсикм процессом мы будем иметь в виду именно такие процессы.

\section{Марковские процессы с дискретным временем и конечным числом состояний}

Время у нас дискретное, то есть процесс у нас описиывается последовательностью состояний $\{X_t\}_{t \in \N_0}$. У нас есть конечное число состояний. Так как нам неважно, как называть состояния, давайте мы их просто как-то пронумеруем от $1$ до $m$. Тогда мы можем считать, что все $X_t$ --- это случайные величины, принимающие значения из $[1..m]$.

В этих условиях свойства Марковского процесса могут быть записаны в более простом виде, причем нам хватит этих свойств для $\delta = 1$.
\begin{enumerate}
  \item \emph{Однородность}: вероятность перехода из состояния $i$ в состояние $j$ за один шаг не зависит от времени.
  \begin{align*}
    p_{ij} \coloneqq& \Pr[X_1 = j \mid X_0 = i] \\
           =& \Pr[X_{n + 1} = j \mid X_n = i].
  \end{align*}
  \item \emph{Марковское свойство}: вероятность перехода зависит только от текущего состояния, а не от всех предыдущих.
  \begin{align*}
    p_{ij} &= \Pr[X_{n + 1} = j \mid X_n = i] \\
           &= \Pr[X_{n + 1} = j \mid X_n = i, X_{n -1}, \dots, X_0].
  \end{align*}
  \item Удовлетворение совйств вероятностной меры: сумма исходящих вероятностей перехода должна равняться единице (иначе куда мы попадем с оставшейся вероятностью?)
  \begin{align*}
    \sum_{j = 1}^m p_{ij} = 1.
  \end{align*}
\end{enumerate}

Мы чуть позже объясним, почему этого хватает для определения марковского процесса. А пока поговорим о его визуализации, то есть марковских цепях.

\subsection{Цепи Маркова}

Мы уже поняли, что для определения марковского процесса нам достаточно определить следующие вещи:
\begin{enumerate}
  \item Множество состояний $S$
  \item Вероятности перехода между состояниями за одну единицу времени $p_{ij}$
\end{enumerate}

Это очень удобно визуализировать с помощью цепей Маркова. Цепь Маркова --- это ориентированный граф без параллельных ребер, но, вероятно, с петлями, где множество вершин соответствует множеству состояний процесса, а ребра --- ненулевым вероятностям перехода.

Рассмотрим на примере очереди в супермаркете. Пусть появления новых покупателей, которые встают в конец очереди, описывается процессом Бернулли с вероятностью $p$. При этом время обслуживания каждого покупателя следует геометрическому распределению с параметром $q$. Также допустим, что если в очереди уже 10 покупателей, то новые покупатели не встают в очередь. Таким образом, в каждый интервал времени происходит одно из следующих событий:
\begin{enumerate}
  \item Пришел новый покупатель, но мы не обслужили текущего. Очередь выросла на 1, если в ней меньше 10 человек.
  \item Мы обслужили текущего покупателя, а новый не появился. Очередь уменьшилась на 1.
  \item Пришел новый покупатель, и мы закончили обслуживание текущего. Очередь не изменилась.
  \item Ничего не произошло. Очередь не изменилась.
\end{enumerate}
В силу определения прихода покупателей и времени обслуживания покупателей, мы можем считать, что в каждый тайм-слот мы кидаем две нечестных монетки с вероятностями успеха $p$ и $q$. Если первая монетка упала успехом --- к нам приходит покупатель. Если вторая монетка упала успехом --- мы заканчиваем обслуживание текущего покупателя. Поэтому вероятность каждого из перечисленных событий следующая:
\begin{enumerate}
  \item $p(1 - q)$
  \item $q(1 - p)$, если очередь не пустая, иначе ноль.
  \item $pq$
  \item $(1 - p)(1 - q)$
\end{enumerate}
Отсюда мы можем нарисовать такой граф:

\begin{center}
  \begin{tikzpicture}
    \node (s0) [draw, circle] at (0, 0) {$0$};
    \node (s1) [draw, circle] at (2, 0) {$1$};
    \node (s2) [draw, circle] at (4, 0) {$2$};
    \node (s3) [draw, circle] at (6, 0) {$3$};
    
    \node at (7.5, 0) {\Huge{$\cdots$}};

    \node (s9) [draw, circle] at (9, 0) {$9$};
    \node (s10) [draw, circle] at (11, 0) {$10$};
    
    \draw [->, ultra thick, red] (s0) to[out=30,in=150] node [midway, above] {$p$} (s1);
    \draw [->, thick] (s1) to[out=30,in=150] (s2);
    \draw [->, ultra thick, red] (s2) to[out=30,in=150] node [midway, above] {\small{$p(1 - q)$}} (s3);
    \draw [thick] (s3) to[out=30,in=180] (7, 0.4);
    \draw [->, thick] (8, 0.4) to[out=0,in=150] (s9);
    \draw [->, thick] (s9) to[out=30,in=150] (s10);

    \draw [->, ultra thick, blue] (s10) to[out=210,in=-30] node [midway, below] {$q$} (s9);
    \draw [thick] (s9) to[out=210,in=0] (8, -0.4);
    \draw [->, thick] (7, -0.4) to[out=180,in=-30] (s3);
    \draw [->, thick] (s3) to[out=210,in=-30] (s2);
    \draw [->, ultra thick, blue] (s2) to[out=210,in=-30] node [midway, below] {$q(1 - p)$} (s1);
    \draw [->, thick] (s1) to[out=210,in=-30] (s0);

    \draw [->, ultra thick, purple] (s0) to[out=120,in=180] (0, 1.5) to[out=0,in=60] (s0);
    \node [above, purple] at (0, 1.5) {$1 - p$};
    \draw [->, thick] (s1) to[out=120,in=180] (2, 1.5) to[out=0,in=60] (s1);
    \draw [->, thick] (s3) to[out=120,in=180] (6, 1.5) to[out=0,in=60] (s3);
    \draw [->, ultra thick, purple] (s2) to[out=120,in=180] (4, 1.5) to[out=0,in=60] (s2);
    \node [above, purple] at (4, 1.5) {$pq + (1 - p)(1 - q)$};
    \draw [->, thick] (s9) to[out=120,in=180] (9, 1.5) to[out=0,in=60] (s9);
    \draw [->, ultra thick, purple] (s10) to[out=120,in=180] (11, 1.5) to[out=0,in=60] (s10);
    \node [above, purple] at (11, 1.5) {$1 - q$};
  \end{tikzpicture}
\end{center}

С помощью таких графов очень удобно описывать марковские процессы, чем мы и будем заниматься в дальнейшем. Подчеркнем еще раз, что если вероятность перехода из состояния $i$ в состояние $j$ равна нулю, то соответствующего ребра нет в цепи Маркова.

\section{Переход через время $n$}

Чтобы показать, что нам хватает условий на переходы через единицу времени для того, чтобы процесс был марковским в общем смысле этого слова, давайте рассмотрим $r_{ij}(n)$ --- вероятность того, что мы попадем в состояние $j$ из состояния $i$ за $n$ шагов. Как его посчитать?
\begin{itemize}
  \item Для $n = 0$ вероятность того, что мы поменяем состояние есть ноль. Поэтому $r_{ii}(0) = 1$ и $r_{ij}(0) = 0$ для всех $i \ne j$.
  \item Для $n = 1$ переходы нам уже известны. $r_{ij}(1) = p_{ij}$.
  \item Чтобы определить оставшиеся, можем последовать по одному из трех вариантов. Но все они основаны на том, что мы суммируем вероятности различных непересекающихся путей длины $n$ в графе из состояния $i$ в состояние $j$.
  \begin{enumerate}
    \item Этот вариант используется чаще всего. Мы можем разбить все пути длины $n$ по тому, какое в них $n-1$-ое состояние. Рассмотрим группу путей, где это состояние есть $k$. Тогда вероятность этой группы есть
    \[
      r_{ik}(n-1) \cdot p_{kj},
    \]
    так как вероятность последнего перехода не зависит от всех предыдущих, а только от состояния в мемент времени $n - 1$. Так как пути из разных групп не пересекаются, то мы можем просто сложить вероятности этих групп:
    \[
      r_{ij}(n) = \sum_{k = 1}^m r_{ik}(n-1) \cdot p_{kj}.
    \]
    \begin{center}
      \begin{tikzpicture}
        \node (i) [draw, circle] at (0, 0) {$i$};
        \node (j) [draw, circle] at (7, 0) {$j$};
        \node (k1) [draw, circle] at (5, 3) {$k_1$};
        \node (k2) [draw, circle] at (5, 1) {$k_2$};
        \node (km) [draw, circle] at (5, -2) {$k_m$};

        \draw [snake=coil, ->] (i) -- (k1) node [pos=0.5, above, sloped] {$n - 1$ шаг};
        \draw [snake=coil, ->] (i) -- (k2) node [pos=0.5, above, sloped] {$n - 1$ шаг};
        \draw [snake=coil, ->] (i) -- (km) node [pos=0.5, above, sloped] {$n - 1$ шаг};

        \draw [->] (k1) -- (j);
        \draw [->] (k2) -- (j);
        \draw [->] (km) -- (j);
      \end{tikzpicture}
    \end{center}
    \item Мы также можем разбить все пути на группы по тому, какое состояние было в пути вторым. Группы не пересекаются, поэтому опять можем просто сложить такие вероятности:
    \[
      r_{ij}(n) = \sum_{k = 1}^m p_{ik} r_{kj}(n-1).
    \]
    \begin{center}
      \begin{tikzpicture}
        \node (i) [draw, circle] at (0, 0) {$i$};
        \node (j) [draw, circle] at (7, 0) {$j$};
        \node (k1) [draw, circle] at (2, 3) {$k_1$};
        \node (k2) [draw, circle] at (2, 1) {$k_2$};
        \node (km) [draw, circle] at (2, -2) {$k_m$};

        \draw [snake=coil, ->] (k1) -- (j) node [pos=0.5, above, sloped] {$n - 1$ шаг};
        \draw [snake=coil, ->] (k2) -- (j) node [pos=0.5, above, sloped] {$n - 1$ шаг};
        \draw [snake=coil, ->] (km) -- (j) node [pos=0.5, above, sloped] {$n - 1$ шаг};

        \draw [->] (i) -- (k1);
        \draw [->] (i) -- (k2);
        \draw [->] (i) -- (km);
      \end{tikzpicture}
    \end{center}
    \item Да и вообще можем разбить на группы в зависимости от состояния на любом шаге $d \in [1..n-1]$ и посчитать  
    \[
      r_{ij}(n) = \sum_{k = 1}^m r_{ik}(d) r_{kj}(n-d).
    \]
    \begin{center}
      \begin{tikzpicture}
        \node (i) [draw, circle] at (0, 0) {$i$};
        \node (j) [draw, circle] at (7, 0) {$j$};
        \node (k1) [draw, circle] at (3.5, 3) {$k_1$};
        \node (k2) [draw, circle] at (3.5, 1) {$k_2$};
        \node (km) [draw, circle] at (3.5, -2) {$k_m$};


        \draw [snake=coil, ->] (i) -- (k1) node [pos=0.5, above, sloped] {$d$ шагов};
        \draw [snake=coil, ->] (i) -- (k2) node [pos=0.5, above, sloped] {$d$ шагов};
        \draw [snake=coil, ->] (i) -- (km) node [pos=0.5, above, sloped] {$d$ шагов};

        \draw [snake=coil, ->] (k1) -- (j) node [pos=0.5, above, sloped] {$n - d$ шагов};
        \draw [snake=coil, ->] (k2) -- (j) node [pos=0.5, above, sloped] {$n - d$ шагов};
        \draw [snake=coil, ->] (km) -- (j) node [pos=0.5, above, sloped] {$n - d$ шагов};
      \end{tikzpicture}
    \end{center}
  \end{enumerate}
\end{itemize}

\subsection{Стартовое состояние и матрица переходов}

Стартовое состояние $X_0$ в марковском процессе бывает либо случайным, либо детерминированным. Однако даже в во втором случае удобно считать, что начальное состояние случайно, просто оно равно какому-то конкретному состоянию с вероятностью 1.

Так как у нас есть $m$ состояний, мы можем задать стохастический вектор $\pi^*$ длины $m$. В этом векторе все элементы удовлетворяют равенству $\pi^*_i = \Pr[X_0 = i]$. Мы упомянули, что этот вектор \emph{стохастический}, то есть все его элементы неотрицательны, и их сумма равна единице.

После введения этого вектора нам также удобно рассмотреть матрицу переходов
\begin{align*}
  P = (p_{ij})_{i = 0..m}^{j = 0..m} = \begin{pmatrix}
    p_{00} & p_{01} & \dots & p_{0m}\\
    p_{10} & p_{11} & \dots & p_{1m}\\
    \cdots & \cdots & \cdots & \cdots \\
    p_{m0} & p_{m1} & \dots & p_{mm}\\
    \end{pmatrix}.
\end{align*}
Опять, мы говорим, что матрица \emph{стохастическая}, если ее строчки являются стохастическими векторами.

Какова вероятность оказаться в стостоянии $j$ после первого шага процесса? По формуле полной вероятности это
\begin{align*}
  \sum_{i = 1}^m \Pr[X_0 = i] \Pr[X_1 = j \mid X_0 = i] = \sum_{i = 1}^m \pi^*_i p_{ij}
\end{align*}
Другими словами, эти вероятности образуют вектор, равный $\pi^* P$. Легко показать, что через $t$ шагов вероятности быть в каждом состоянии цепи склыдваются в вектор $\pi^* P^t$. При этом элементы матрицы $P^t$ равны $r_{ij}(t)$. Все это громоздкий, но тривиальный ЛинАл, поэтому мы его опустим.

\section{Возвратные и невозвратные состояния. Периодические состояния}

\emph{Возвратным} называется такое сотояние, что если мы в нем находимся, то с вероятностью 1 мы посетим его еще раз в будущем. Другими словами, это такие состояния, для которых если есть путь в какое-то состояние $i$, то из этого состояния точно есть путь обратно. \emph{Невозвратными} называются все остальные состояния.

Рассмотрим на примере. На приведенном ниже рисунке синим обозначены возвратные состояния, а красным --- невозвратные.

\begin{center}
  \begin{tikzpicture}
    \node (s1) [draw, circle, fill=blue!50!lightgray] at (0, 0) {$1$};
    \node (s2) [draw, circle, fill=blue!50!lightgray] at (0, 3) {$2$};
    \node (s3) [draw, circle, fill=blue!50!lightgray] at (3, 3) {$3$};
    \node (s4) [draw, circle, fill=red!50!lightgray] at (3, 0) {$4$};
    \node (s5) [draw, circle, fill=red!50!lightgray] at (6, 3) {$5$};
    \node (s6) [draw, circle, fill=red!50!lightgray] at (6, 0) {$6$};
    \node (s7) [draw, circle, fill=blue!50!lightgray] at (8, 1) {$7$};
    \node (s8) [draw, circle, fill=blue!50!lightgray] at (11, 1) {$8$};
    \node (s9) [draw, circle, fill=red!50!lightgray] at (11, 3) {$9$};
    
    \draw [->, thick] (s5) -- (s3);
    \draw [->, thick] (s4) -- (s1);
    \draw [->, thick] (s4) -- (s5);
    \draw [->, thick] (s5) -- (s6);
    \draw [->, thick] (s6) -- (s7);
    \draw [->, thick] (s9) -- (s8); 

    \draw [->, thick] (s1) to[out=120,in=-120] (s2);
    \draw [->, thick] (s2) to[out=-60,in=60] (s1); 
    \draw [->, thick] (s3) to[out=-150,in=-30] (s2);
    \draw [->, thick] (s2) to[out=30,in=150] (s3); 
    \draw [->, thick] (s6) to[out=-150,in=-30] (s4);
    \draw [->, thick] (s4) to[out=30,in=150] (s6); 
    \draw [->, thick] (s8) to[out=-150,in=-30] (s7);
    \draw [->, thick] (s7) to[out=30,in=150] (s8); 

    
    \draw [->, thick] (s2) to[out=-150,in=-90] (-2, 3) to[out=90,in=150] (s2);
    \draw [->, thick] (s1) to[out=-150,in=-90] (-2, 0) to[out=90,in=150] (s1);
    \draw [->, thick] (s7) to[out=120,in=180] (8, 3) to[out=0,in=60] (s7);

    \draw [purple, ultra thick] (2, 0) -- (5, 3) to[out=45,in=0] (4, 4) -- node [above] {Возвратный класс 1} (0, 4) to[out=180,in=90] (-1, 3) -- (-1, 0) to[out=-90,in=180] (0, -1) -- (1, -1) to[out=0,in=-135] (2, 0);

    \draw [purple, ultra thick] (8, 0) -- node [below] {Возвратный класс 2} (11, 0) to[out=0,in=-90] (12, 1) to[out=90, in=0] (11, 2) -- (8, 2) to[out=180,in=90] (7, 1) to[out=-90,in=180] (8, 0);
  \end{tikzpicture}
\end{center}

Возвратный класс --- это множество достижимых друг из друга возвратных состояний. На рисунке можно выделить два класса.

Возвратный класс может быть \emph{периодическим}. Это значит, что для какого-то $d > 1$ его можно разбить на $d$ групп состояний, таких, что все переходы из одной группы ведут в какую-то одну другую группу. На рисунке представлен периодический возвратный класс с $d = 2$.
\begin{center}
  \begin{tikzpicture}
    \draw (0, 0) circle (1mm);
    \draw (0, 0.5) circle (1mm);
    \draw (0.3, 0.2) circle (1mm);
    \draw (0.5, 0.1) circle (1mm);
    \draw (0.4, 0.4) circle (1mm);
    \draw [ultra thick, purple, rounded corners] (-0.2, -0.2) rectangle (0.7, 0.7);

    \draw (0.5, -1) circle (1mm);
    \draw (0.5, -1.5) circle (1mm);
    \draw (0.3, -1.2) circle (1mm);
    \draw (0, -1.1) circle (1mm);
    \draw (0.2, -1.4) circle (1mm);
    \draw [ultra thick, purple, rounded corners] (-0.2, -0.8) rectangle (0.7, -1.7);

    \draw [ultra thick, ->] (0.5, 0.5) to[out=0,in=90] (2, -0.5) to[out=-90,in=0] (0.5, -1.5);
    \draw [ultra thick, ->] (0.5, 0.1) to[out=0,in=90] (1.6, -0.5) to[out=-90,in=0] (0.5, -1.1);

    \draw [ultra thick, <-] (0, 0.5) to[out=180,in=90] (-1.5, -0.5) to[out=-90,in=180] (0, -1.5);
    \draw [ultra thick, <-] (0, 0.1) to[out=180,in=90] (-1.1, -0.5) to[out=-90,in=180] (0, -1.1);
  \end{tikzpicture}
\end{center}
Рисунок с тремя классами проще нарисовать на доске.

\section{Стационарное распределение}

Рассмотрим следующую простую цепь:
\begin{center}
  \begin{tikzpicture}
    \node (s1) [draw,circle] at (0, 0) {$1$};
    \node (s2) [draw,circle] at (4, 0) {$2$};

    \draw [ultra thick, ->] (s1) to[out=30,in=150] node [above] {$0.5$} (s2);
    \draw [ultra thick, ->] (s2) to[out=-150,in=-30] node [below] {$0.2$} (s1);
    
    \draw [ultra thick, ->] (s1) to[out=120,in=180] (0, 2) to[out=0,in=60] (s1);
    \node [above] at (0, 2) {$0.5$};
    \draw [ultra thick, ->] (s2) to[out=120,in=180] (4, 2) to[out=0,in=60] (s2);
    \node [above] at (4, 2) {$0.8$};
  \end{tikzpicture}
\end{center}

Попробуем посчитать $r_{ij}(t)$

\begin{center}
  \begin{tabular}{c|c|c|c|c|c|}
    $t$ & $0$ & $1$ & $\dots$ & $100$ & $101$ \\ \hline
    $r_{11}(t)$ & $1$ & $0.5$ & & $\approx \frac{2}{7}$ & $\approx \frac{2}{7}$ \\ \hline
    $r_{12}(t)$ & $0$ & $0.5$ & & $\approx \frac{5}{7}$ & $\approx \frac{5}{7}$ \\ \hline
    $r_{21}(t)$ & $0$ & $0.2$ & & $\approx \frac{2}{7}$ & $\approx \frac{2}{7}$ \\ \hline
    $r_{22}(t)$ & $1$ & $0.8$ & & $\approx \frac{5}{7}$ & $\approx \frac{5}{7}$ \\ \hline
  \end{tabular}
\end{center}

Заметим, что независимо от начального состояния $X_0$ вероятность находиться в каком-то конкретном состоянии стремится к какому-то числу. Всегда ли это так? Не всегда, вот контрпримеры.

\textbf{Пример 1.} В данной цепи в случае $X_0 = 2$ вероятность находиться в состоянии $2$ через четное число шагов есть ноль, а через нечетное есть единица.
\begin{center}
  \begin{tikzpicture}
    \node (s1) [draw,circle] at (0, 0) {$1$};
    \node (s2) [draw,circle] at (3, 0) {$2$};
    \node (s3) [draw,circle] at (6, 0) {$3$};


    \draw [ultra thick, ->] (s1) to[out=30,in=150] node [above] {$1$} (s2);
    \draw [ultra thick, ->] (s2) to[out=-150,in=-30] node [below] {$0.5$} (s1);

    \draw [ultra thick, ->] (s2) to[out=30,in=150] node [above] {$0.5$} (s3);
    \draw [ultra thick, ->] (s3) to[out=-150,in=-30] node [below] {$1$} (s2);
    
  \end{tikzpicture}
\end{center}

\textbf{Пример 2.} В данной цепи пределы $r_{i3}(t)$ очень разнятся:
\begin{itemize}
  \item $r_{13}(t) \to 0$
  \item $r_{23}(t) \to 0.5$
  \item $r_{33}(t) \to 1$
\end{itemize}

\begin{center}
  \begin{tikzpicture}
    \node (s1) [draw,circle] at (0, 0) {$1$};
    \node (s2) [draw,circle] at (3, 0) {$2$};
    \node (s3) [draw,circle] at (6, 0) {$3$};


    \draw [ultra thick, ->] (s2) -- node [above] {$0.5$} (s3);
    \draw [ultra thick, ->] (s2) -- node [above] {$0.5$} (s1);

    
    \draw [ultra thick, ->] (s1) to[out=120,in=180] (0, 2) to[out=0,in=60] (s1);
    \node [above] at (0, 2) {$1$};
    \draw [ultra thick, ->] (s3) to[out=120,in=180] (6, 2) to[out=0,in=60] (s3);
    \node [above] at (6, 2) {$1$};
    
  \end{tikzpicture}
\end{center}

\begin{theorem}
  Если в цепи Маркова есть только один возвратный класс, и он не периодичен, то существует стохастический вектор $\pi$, такой что для всех $i, j \in [1..m]$
  \[
    \lim_{t \to \infty} r_{ij}(t) = \pi_j
  \]
\end{theorem}

Мы опустим доказательство (так как оно требует функана и линала) или обсудим его детали позже.

Но рассмотрим такое следствие для цепей Маркова, удовлетворяющим условиям теоремы. У нас есть такое рекурентное соотношение:
\begin{align*}
  r_{ij}(t) = \sum_{k = 1}^m r_{ik}(t - 1) p_{ij}.
\end{align*}
Если мы сделаем предельный переход при $t \to + \infty$, то получим:
\begin{align*}
  \pi_j = \sum_{k = 1}^m \pi_k p_{kj}.
\end{align*}
То есть для нахождения всех $\pi_j$ нам надо решить систему из $m$ линейных уравнений. Но эта система имеет бесконечно много решений: если мы нашли какой-то ненулевой вектор $\pi$, то мы можем его также умножить на константу, и он по-прежнему будет удовлетворять систему уравнений.  Поэтмоу нужно дополнительное условие:
\begin{align*}
  \sum_{i = 1}^m \pi_i = 1,
\end{align*}
которое следует предельным переходом из равенства
\begin{align*}
  \sum_{i = 1}^m r_{ji}(t) = 1.
\end{align*}

Вектор $\pi$ часто называют \emph{стационарным распределением}.

\section{Частота посещений и переходов}

Элементы вектора $\pi$ можно интерпретировать следующим образом. Раз для довольно больших $t$ веротяность находиться в одном конкретном состоянии $i$ близка к $\pi_i$, то мы можем рассматривать посещения этого состояния как процесс Бернулли с параметром $\pi_i$. То есть для наблюдателя, поставленного в состоянии $i$ за долгий преиод времени будет выполняться следующее:
\begin{align*}
  \frac{Y_i(t)}{t} \approx \pi_i,
\end{align*}
где $Y_i(t)$ --- число посещений состояния $i$ за первые $t$ шагов.

Мы также можем поставить наблюдателя на конкретный переход $i \to j$. Этот переход может быть использован только когда мы посетили состояние $i$, а потом из всех переходов выбрали его. Поэтому если $Z_{ij}(t)$ --- число посещений перехода $i \to j$ за время $t$, то при больших $t$ мы будем иметь
\begin{align*}
  \frac{Z_{ij}(t)}{t} \approx \pi_i p_{ij}.
\end{align*}

Знание частоты переходов помогает анализировать некоторые процессы, например, так называемый демографический процесс, отображающий численность популяции, которая не может превысить какую-то предельную численность. Примером такого процесса как раз является очередь в супермаркете. Рассмотрим такую марковскую цепь:

\begin{center}
  \begin{tikzpicture}
    \node (s0) [draw, circle] at (0, 0) {$0$};
    \node (s1) [draw, circle] at (2, 0) {$1$};
    \node (s2) [draw, circle] at (4, 0) {$2$};
    \node (s3) [draw, circle] at (6, 0) {$3$};
    
    \node at (7.5, 0) {\Huge{$\cdots$}};

    \node (s9) [draw, circle] at (9, 0) {$m - 1$};
    \node (s10) [draw, circle] at (11, 0) {$m$};
    
    \draw [->, thick] (s0) to[out=30,in=150] node [midway, above] {$p_0$} (s1);
    \draw [->, thick] (s1) to[out=30,in=150] node [midway, above] {$p_1$}(s2);
    \draw [->, thick] (s2) to[out=30,in=150] node [midway, above] {\small{$p_2$}} (s3);
    \draw [thick] (s3) to[out=30,in=180] (7, 0.4);
    \draw [->, thick] (8, 0.4) to[out=0,in=150] (s9);
    \draw [->, thick] (s9) to[out=30,in=150] node [midway, above] {$p_{m - 1}$} (s10);

    \draw [->, thick] (s10) to[out=210,in=-30] node [midway, below] {$q_m$} (s9);
    \draw [thick] (s9) to[out=210,in=0] (8, -0.4);
    \draw [->, thick] (7, -0.4) to[out=180,in=-30] (s3);
    \draw [->, thick] (s3) to[out=210,in=-30] node [midway, below] {$q_3$}  (s2);
    \draw [->, thick] (s2) to[out=210,in=-30] node [midway, below] {$q_2$} (s1);
    \draw [->, thick] (s1) to[out=210,in=-30] node [midway, below] {$q_1$}  (s0);

    \draw [->, thick] (s0) to[out=120,in=180] (0, 1.5) to[out=0,in=60] (s0);
    % \node [above] at (0, 1.5) {$1 - p$};
    \draw [->, thick] (s1) to[out=120,in=180] (2, 1.5) to[out=0,in=60] (s1);
    \draw [->, thick] (s3) to[out=120,in=180] (6, 1.5) to[out=0,in=60] (s3);
    \draw [->, thick] (s2) to[out=120,in=180] (4, 1.5) to[out=0,in=60] (s2);
    % \node [above] at (4, 1.5) {$pq + (1 - p)(1 - q)$};
    \draw [->, thick] (s9) to[out=120,in=180] (9, 1.5) to[out=0,in=60] (s9);
    \draw [->, thick] (s10) to[out=120,in=180] (11, 1.5) to[out=0,in=60] (s10);
    % \node [above] at (11, 1.5) {$1 - q$};
  \end{tikzpicture}
\end{center}

Давайте посмотрим частоты переходов из состояния $i$ в состояние $i + 1$ и обратно. Логично, что они отличаются не более, чем на $\frac{1}{t}$, при этом равны $\pi_i p_i$ и $\pi_{i + 1}q_{i + 1}$ соответственно. Таким образом,

\begin{align*}
  \pi_i p_i &= \pi_{i + 1}q_{i + 1} \\
  \pi_{i + 1} = \pi_i \frac{p_i}{q_{i + 1}}.
\end{align*}

То есть мы последовательно можем вычислить все $\pi_i$ из $\pi_0$ Но как найти $\pi_0$? Да просто из нормировки.

\begin{align*}
  \sum_{i = 0}^m \pi_i &= \sum_{i = 0}^m \pi_0 \prod_{j = 0}^i  \frac{p_j}{q_{j + 1}} = \pi_0 \sum_{i = 0}^m \prod_{j = 0}^i  \frac{p_j}{q_{j + 1}} = 1.
\end{align*}

Рассмотрим особый случай, когда у нас все $p_i = p$ и все $q_i = q$. Пусть $\rho = \frac{p}{q}$. В таком случае, можно сказать, что 
\begin{align*}
  \pi_i &= \pi_{i - 1}\rho = \pi_0 \rho^i \\
  \sum_{i = 0}^m \pi_i &= \sum_{i = 0}^m \pi_0 \rho^i = \pi_0 \frac{1 - \rho^{m - 1}}{1 - \rho} = 1 \\
  \pi_0 &= \frac{1 - \rho}{1 - \rho^{m - 1}},
\end{align*}
если $\rho \ne 1$ и
\begin{align*}
  \sum_{i = 0}^m \pi_i &= \sum_{i = 0}^m \pi_0 \rho^i = \pi_0 (m + 1) = 1 \\
  \pi_0 &= \frac{1}{m + 1},
\end{align*}

В случае, когда $p < q$ (то есть $\rho < 1$), а $m$ достаточно велик, чтобы мы могли пренебречь $\rho^{m - 1}$, то мы получаем:
\begin{align*}
  \pi_0 &= 1 - \rho.
\end{align*}
Заметим, что при этом вероятность того, что размер популяции равен $i$, падает экспоненциально с ростом $i$. И мы можем легко посчитать ожидаемый размер популяции в 
далеком будущем, то есть когда распределение станет достаточно близким к $\pi$:
\begin{align*}
  E[X_t] &\approx \sum_{i = 0}^m i \pi_i = \sum_{i = 0}^m i (1 - \rho) \rho^i = E[\Geom(1 - \rho) - 1] = \frac{\rho}{1 - \rho}.
\end{align*}

\end{document}