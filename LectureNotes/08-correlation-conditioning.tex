\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\eps{\varepsilon}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}


\title{Лекция 8. Коэффициент корреляции. Условные матожидание и дисперсия как случайные величины}

\begin{document}
\maketitle


\section{Коэффициент корреляции}

Ковариация -- очень странная величина. Ее размер по сути зависит от разброса двух с.в., котороые являются ее аргументом. То есть по фразе ``ковариация $X$ и $Y$ равна единице'' очень сложно сказать, много это, или мало, так как мы не знаем масштаба $X$ и $Y$. Например, когда мы пытаемся посчитать ковариацию температуры воздуха и давления, то в зависимости от используемых величин измерения (цельсий или фаренгейты, паскали или атмосферы) будет разная ковариация. Причем у самой ковариации всегда есть единица измерения, равная единице измерения $X$, умноженная на $Y$ (например, градус цельсия * паскаль), что еще больше усложняет осознание степени зависимости пары с.в. Поэтому была введена мера зависимости с.в., называемая коэффициентом корреляции $\rho(X, Y)$.

\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \rho(X, Y) = \frac{\Cov(X, Y)}{\sigma_X \sigma_Y}
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

\emph{NB:} он определен только для с.в. с ненулевой дисперсией. 

Его основные свойсва:

\begin{enumerate}
  \item $\rho(X, Y)\in [-1, 1]$
  \item Если $X$ и $Y$ независимы, то $\rho(X, Y) = 0$, как и ковариация (обратное не всегда верно, см. пример для нулевой ковариации)
  \item Если $|\rho(X, Y)| = 1$, то $X$ и $Y$ линейно зависимы, то есть $(X - E[X]) = c(Y - E[Y])$. Как следствие: $\rho(X, X) = \frac{\Cov(X, X)}{\sigma_X^2} = 1$.
  \item Линейные преобразования любой из двух с.в. не меняют модуль коэффициента корреляции: $\rho(aX + b, Y) = \frac{a\Cov(X, Y)}{|a|\sigma_X \sigma_Y} = \sign(a)\rho(X, Y)$
\end{enumerate}

Для доказательства первого и третьего свойств рассмотрим с.в. $X$ и $Y$ с нулевыми матожиданиями и единичной дисперсией (для общего случая легко доказать все, перейдя к с.в. $X' = \frac{X - E[X]}{\sigma_X}$ и $Y' = \frac{Y - E[Y]}{\sigma_Y}$, но мы опустим это для упрощения вычислений). Рассмотрим с.в. $Z = (X - \rho(X, Y)Y)^2$. Она неотрицательна, значит, и ее матожидание неотрицательно.

\begin{align*}
  E[Z] &= E[(X - \rho Y)^2] = E[X^2] - 2\rho E[XY] + \rho^2 E[Y^2] \\
       &= \Var(X) - 2 \rho \cdot \rho + \rho^2 \Var(Y) = 1 - \rho^2 \ge 0   
\end{align*}

Отсюда следует, что во-первых, $\rho^2 \le 1$, а во-вторых, если $|\rho| = 1$, то $E[Z] = 0$, что возможно только если $Z$ всегда равен нулю, что в свою очередь подразумевает, что $X$ всегда равен $\pm Y$ (в зависимости от знака $\rho$).

\section{Интерпретация корреляции}

Если две случайных величины зависимы, то чаще всего это не значит, что одна определяет другую и наоборот. Например, проведя исследование оценок в школе, можно обнаружить, что у учеников с хорошей оценкой по математике также хорошие оценки по литературе, однако это не значит, что знания по математике помогают в изучении литературы и наоборот. Это скорее означает, что у двух этих случайных величин есть какой-то общий фактор, который влияет на них обеих. В случае с оценками это может быть, например, степень усердия, которое тот или иной ученик прилагает к учебе. 

Чуть более формально, давайте представим, что есть три случайных величины $Z, U$ и $V$ c нулевыми матожиданиями и единичными дисперсиями, которые все независимы друг от друга. И рассмотрим две других с.в. $X = Z + U$ и $Y = Z + V$. Посчитаем $\rho(X, Y)$

\begin{align*}
  \Var(X) &= \Var(Y) = \Var(Z) + \Var(U) = 2 \\
  \sigma_X &= \sigma_Y = \sqrt(2) \\
  E[XY] &= E[Z^2 + UZ + VZ + UV] = \Var(Z) = 1 \\
  \rho(X, Y) &= \frac{E[XY]}{\sigma_X \sigma_Y} = \frac{1}{2}
\end{align*}

То есть коэффициент корреляции определяет вклад какой-то причины, которая влияет на обе с.в., в каждую с.в.

\section{Пример важности корреляции}

Допустим, вы решили инвестировать. И у вас есть 10\$, и вы вкладываете по доллару в какие-то 10 компаний. Средняя выручка с каждой компании равна 1\$ и среднеквадратичное отклонение выручки с каждой компании есть $1.3$\$. Очевидно, ваша суммарная выручка равна вашему вкладу, но каково среднеквадратичное отклонение? Если выручки от разных компаний независимы, То
\begin{align*}
  \Var(X_1 + \dots + X_{10}) &= \sum_{i = 1}^{10} \Var(X_i) = 10 \cdot 1.3^2 = 16.9 \\
  \sigma_{X_1 + \dots + X_10} &= \sqrt{16.9} \approx 4.1
\end{align*}
То есть сумма довольно хорошо сконцентрирована, больше, чем при вкладе всего лишь в одну компанию. На большую прибыль надеяться не стоит, но и много проиграть шансов не так много. Теперь допустим, что у всех выручек есть какой-то общий фактор, который под ними лежит, и коэффициент корелляции между любыми $X_i$ и $X_j$ равен 0.9. Тогда
\begin{align*}
  \Var(X_1 + \dots + X_10) &= \sum_{i = 1}^{10} \Var(X_i) - \sum_{i \ne j} \Cov(X_i, X_j) = 10 \cdot 1.3^2  + 90 \cdot 0.9 \cdot (1.3)^2 \approx 154 \\
  \sigma_{X_1 + \dots + X_10} &= \sqrt{16.9} \approx 12.4
\end{align*}
То есть если падают акции хотя бы одной компании, то с большой вероятностью падают и акции других, что приводит к большим потерям. Примерно это и случилось в 2008 году, когда рухнул весь рынок сразу.

\section{Уточнение про типы с.в.}

Какие бывают с.в.:
\begin{itemize}
  \item Сингулярные --- сконцентрированные на множестве меры ноль (имеется в виду мера Лебега на $\R$)
  \item Абсолютно непрерывные --- те, у которых есть плотность вероятности
  \item Смешанные --- те, функция распределения которых является взвешенной суммой функций распределений какой-то сингулярной и какой-то непрерывной с.в. То есть для любой с.в. $X$ сущестуют такие с.в. $Y$ и $Z$ ($Y$ --- сингулярная, $Z$ --- абсолютно непрерывная) и число $p\in[0, 1]$, что
  \[
    F_X = pF_Y + (1 - p)F_Z,
  \] 
  что является равенством функций, то есть их значения на всех $x \in \R$ совпадают. То есть с вероятностью $p$ с.в. $X$ принимает значение сингулярной с.в., а с вероятностью $(1 - p)$ --- абсолютно непрерывной
\end{itemize}

Почему мы раньше говорили о дискретных а теперь их не упоминаем, но говорим о каких-то сингулярных? Потому что дискретные с.в. (те, которые имеют функцию вероятностей) есть подмножество сингулярных, но не все сингулярные имеют функцию вероятностей. Пример сингулярного недискретного распределения --- то, у которого функция распределения есть лестница Кантора --- функция, неубывающая и непрерывная на отрезке $[0, 1]$, которая возрастает на нем от $0$ до $1$, но при этом почти всюду (то есть на всех точках, за исключением множества меры ноль) имеет нулевую производную.

Поэтому уместнее разбить сингулярные с.в. на два типа: дискретные (именно в том понимании, в котором мы имели с ними делао) и сингулярно-непрерывные (те, у которых функция распределения возрастает на множестве меры ноль, но не имеют точек разрыва). Таким образом, для любой с.в. $X$ существуют такие с.в. $Y, Z$ и $S$ ($Y$ --- дискретная, $Z$ --- абсолютно непрерывная и $S$ --- сингулярно-непрерывная) и числа $p \in [0, 1], q \in [0, 1 - p]$, что 
\[
  F_X = pF_Y + qF_Z + (1 - p - q)F_S.
\]
Это доказывается довольно сложно и выходит за рамки нашего курса, но если интересно можете почитать про теорему Лебега о разложении меры (Lebesgue's decomposition).

В рамках нашего курса мы не будем сталкиваться с сингулярно-непрерывными распределениями, поэтому мы будем считать, что любая с.в. может быть представлена в виде комбинации дискретной и непрерывной. 

\section{Условное матожидание как с.в.}

Мы до этого рассматривали функции от с.в. и говорили, что они также являются с.в. Например, если есть функция $g(x): \R \to \R$, то мы можем рассмотреть композицию этой функции и с.в. $X$, а именно $g(X): \Omega \to \R \to \R$. 

Вспомним также, что такое условное матожидание. $E[X \mid Y = y]$ -- это число, равное матожиданию с.в. $X$ при условии, что с.в. $Y = y$ (\emph{NB:} это условное матожидание может и не существовать или быть бесконечностью). То есть мы можем рассматривать это как функцию от $y$:
\[
  g(y) = E[X \mid Y = y]
\]
А следовательно, мы можем рассмотреть и композицию этой функции с с.в. $Y$:
\[
  g(Y) = E[X \mid Y],
\]
что является с.в.

А раз это с.в., то можно говорить о ее матожидании, дисперсии и прочих характеристиках.

\section{Матожидание условного матожидания}

Мы рассматриваем с.в. $Z = g(Y) = E[X \mid Y]$. Посчитаем ее матожидание в дискретном случае $Y$.

\begin{align*}
  E[E[X \mid Y]] &= E[g(Y)] = \sum_y p_Y(y) g(y) = \sum_y p_Y(y) E[X \mid Y = y] = E[X],
\end{align*}

где последнее равенство --- по формуле полной вероятности. То есть мы получили:
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        E[E[X \mid Y]] = E[X]
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

В англоязычной литературе это называется Law of iterated expectations, в русской --- просто вариацией теоремы о полном матожидании (что есть правда). Аналогичное равенство можно получить и для непрерывной $Y$, заменив сумму интеграла, а функцию вероятностей --- плотностью.

Вспомним пример про ломание палки из 6-ой лекции (непрерывные с.в, часть 3). Напомним: сначала ломаем палку длины $\ell$ в случайном месте $Y \sim U(0, \ell)$. Потом берем кусок $[0, Y]$ и опять ломаем его в случайном месте $X \sim U(0, Y)$. И найдем матожидание $X$ следующим образом:

\begin{align*}
  E[X] = E[E[X \mid Y]] = E\left[\frac{Y}{2}\right] = \frac{1}{2}E[Y] = \frac{\ell}{4}
\end{align*}

Рассмотрим еще пример, чтобы лучше понять значение этого утверждения. например, вы сейчас хотите сделать прогноз о том, сколько вы заработаете на акциях в течение мая. Пусть это число будет равно $X$, и вас интересует $E[X]$. Но к началу мая сумарная стоимость ваших акций $Y$ будет не такой, как сейчас, то есть она тоже будет с.в. И ваш новый прогноз на прибыл будет $E[X \mid Y = y]$. Однако если вернуться в настоящий момент, то для вас прогноз, сделанный в мае также будет являться случайной величиной. При этом $E[X] = E[E[X \mid Y]]$ можно читать так:

\begin{center}
  \begin{minipage}{0.8\textwidth}
      \textit{Матожидание прогноза, сделанного через месяц, равно матожиданию текущего прогноза.}
  \end{minipage}
\end{center}

\section{Матожидание условной дисперсии}

Для начала ведем понятие условной дисперсии.

\begin{align*}
  \Var(X \mid Y = y) = E\left[(X - E[X \mid Y = y])^2 \mid Y = y\right]
\end{align*}
То есть это вариация, которая посчитана в том мире, где у нас уже произошло событие $Y = y$ и у нас соответсвтующим образом изменилась вероятностная мера для с.в. $X$. Но это опять функция от $Y$, поэтому мы можем снова определить с.в. $Z = \Var(X \mid Y)$ как с.в., которая принимает значение $\Var(X \mid Y = y)$, если $Y$ принимает значение $y$.

Пример с ломанием палки, где $X \sim U(0, Y)$: $\Var(X \mid Y = y) = \frac{y^2}{12}$, при этом $\Var(X \mid Y) = \frac{Y^2}{2}$ --- с.в. Но вот полная дисперсия в отличие от полного матожидания устроена сложнее:

\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \Var(X) = E[\Var(X \mid Y)] + \Var(E[X \mid Y])
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

Давайте докажем это. Посчитаем сначала условную дисперсию при известном значении $Y$:
\begin{align*}
  \Var(X \mid Y = y) = E[X^2 \mid Y = y] - (E[X \mid Y = y])^2
\end{align*}
Так как это равенство выполнено для всех возможных значений $y$, то можем перейти к равенству с.в. (то есть при каждом элементарном исходе они принимают одинаковые значения).
\begin{align*}
  \Var(X \mid Y) = E[X^2 \mid Y] - (E[X \mid Y])^2
\end{align*}
А раз равны с.в., то равны и их матожидания
\begin{align*}
  E[\Var(X \mid Y)] = E[X^2] - E[(E[X \mid Y])^2]
\end{align*}
Но также мы можем посчитать вариацию с.в. $E[X \mid Y]$:
\begin{align*}
  \Var(E[X \mid Y]) = E[(E[X \mid Y])^2] - (E[X])^2
\end{align*}
Остается сложить последние два равенства, тогда справа будет дисперсия $X$, а слева то, что указано в формуле полной дисперсии.

\section{Примеры, если требуются}

Первый пример: $(X, Y)$ такие, что:
\begin{itemize}
  \item с вероятностью $\frac 12$ с.в. $Y = 1,$ а $X \sim U(0, 1)$
  \item с вероятностью $\frac 12$ с.в. $Y = 2,$ а $X \sim U(1, 3)$
\end{itemize}

Можем посчитать:
\begin{itemize}
  \item $E[X \mid Y]$
  \item $E[X] = E[E[X \mid Y]]$
  \item $\Var(X \mid Y)$
  \item $E[\Var(X \mid Y)]$
  \item $\Var(E[X \mid Y])$
\end{itemize}

Более глобальный пример: $Y$ --- номер группы, известны размеры групп, $X$ --- оценка каждого элемента в группе. Известны матожидания и дисперсии $X$ внутри каждой группы.

Можем посчитать:
\begin{itemize}
  \item $E[X \mid Y]$
  \item $E[X] = E[E[X \mid Y]]$
  \item $\Var(E[X \mid Y])$
  \item $\Var(X \mid Y)$
  \item $E[\Var(X \mid Y)]$
\end{itemize}
Можно интерпретировать формулу полной дисперсии как средняя дисперсия внутри групп плюс дисперсия между группами.

\section{Матожидание и дисперсия суммы случайного числа с.в.}

Рассмотрим ситуацию: вы заходите в $N$ магазинов, где $N$ --- некоторая с.в. В $i$-ом магазине вы тратите $X_i$ рублей, причем
\begin{itemize}
  \item Все $X_i$ независимы друг от друга и от $N$ (но $N$ может зависеть от $\{X_i\}$)
  \item Все $X_i$ имеют одинаковое распределение, такое же, как какая-то с.в. $X$.
\end{itemize}
Сколько вы ожидаемо потратили? Пусть $Y = \sum_{i = 1}^N X_i$, тогда 
\begin{align*}
  E[Y \mid N = n] = nE[X]
\end{align*}
Это функция, то есть мы можем задать с.в. $Z = E[Y \mid N] = NE[X]$. А тогда мы знаем, что
\begin{align*}
  E[Y] = E[E[Y \mid N]] = E[NE[X]] = E[N]E[X].
\end{align*}
Заметим, что это верно только если матожидания $N$ и $X$ конечны. Это часто называется равенством Вальда, но его мы еще рассмотрим подробнее позже.
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        E\left[\sum_{i = 1}^{N} X_i\right] = E[N] E[X]
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}


Посчитаем также дисперсию.
\begin{align*}
  \Var(E[Y\mid N]) = \Var(NE[X]) = (E[X])^2 \Var(N),
\end{align*}
так как $E[X]$ не зависит от $N$ и считается константой.
\begin{align}
  \Var(Y \mid N = n) &= \Var\left(\sum_{i = 0}^N X_i \mid N = n\right) = \Var\left(\sum_{i = 0}^n X_i\right) = n\Var(X) \\
  \Var(Y \mid N) &= N\Var(X) \\
  E[\Var(Y \mid N)] &= E[N\Var(X)] = E[N]\Var(X)
\end{align}
Теперь сложим $\Var(E[Y\mid N])$ и $E[\Var(Y \mid N)]$, получим дисперсию $Y$:
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \Var\left(\sum_{i = 1}^{N} X_i\right) = (E[X])^2 \Var(N) + E[N]\Var(X)
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

То есть дисперсия $Y$ есть среднее число слагаемых умножить на дисперсию каждого, плюс она увеличивается дисперсией числа слагаемых, умноженных на квадрат среднего значения каждого слагаемого.


\end{document}