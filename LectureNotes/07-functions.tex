\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\eps{\varepsilon}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}


\title{Лекция 7. Функции от с.в.}

\begin{document}
\maketitle

\section{Линейные преобразования}

Как это в общем случае посчитать новое распределение линейной функции от с.в.? Рассмотрим $Y = aX + b$ (где $a \ne 0$, иначе это скучный случай, когда $Y = b$ с вероятностью 1). Пусть сначала $X$ дискретная, и нам известна ее функция вероятностей. Тогда

\begin{align*}
  p_Y(y) = \Pr(Y = y) = \Pr(aX + b = y) = \Pr\left(X = \frac{y - b}{a}\right) = p_X\left( \frac{y - b}{a}\right)
\end{align*}

С непрерывными такое не работает, так как вероятность, что непрерывная с.в. равна конкретному числу, есть ноль. Но мы можем работать с функциями распределения! Допустим мы знаем $f_X(x)$ и $F_X(x)$. Рассмотрим случай $a > 0$

\begin{align*}
  F_Y(y) &= \Pr(Y \le y) = \Pr(aX + b \le y) = \Pr\left(X \le \frac{y - b}{a}\right) = F_X\left(\frac{y - b}{a}\right), \\
  f_Y(y) &= F_Y'(y) = f_X\left(\frac{y - b}{a}\right) \cdot \frac{1}{a}.
\end{align*}

И отдельно $a < 0$
\begin{align*}
  F_Y(y) &= \Pr(Y \le y) = \Pr(aX + b \le y) = \Pr\left(X \ge \frac{y - b}{a}\right) = 1 - F_X\left(\frac{y - b}{a}\right), \\
  f_Y(y) &= F_Y'(y) = -f_X\left(\frac{y - b}{a}\right) \cdot \frac{1}{a}.
\end{align*}

Объединяя два случая, получаем:

\begin{align*}
  f_Y(y) = \frac{1}{|a|}f_X\left(\frac{y - b}{a}\right).
\end{align*}

Докажем теперь, что линейное преобразование нормального распределения оставляет его нормальным. Пусть $X \sim N(\mu, \sigma^2)$ и $Y = aX + b$. Значит,

\begin{align*}
  f_Y(y) &= \frac{1}{|a|}f_X\left(\frac{y - b}{a}\right) \\
         &= \frac{1}{|a|\sigma \sqrt{2\pi}} \exp\left(\frac{\left(\frac{y - b}{a} - \mu\right)^2}{2\sigma^2}\right) \\
         &= \frac{1}{(|a|\sigma) \sqrt{2\pi}} \exp\left(\frac{\left(y - (b + a\mu)\right)^2}{2(a\sigma)^2}\right),
\end{align*}
что есть функция плотности вероятности для $N(a\mu + b, (a\sigma)^2)$.

\section{Нелинейные преобразования}

Алгоритм поиска нового распределения $Y = g(X)$:
\begin{enumerate}
  \item Найти функцию распределения $F_Y(y) = \Pr(Y \le y) = \Pr(g(X) \le y)$
  \item Продифференцировать: $f_Y(y) = F_Y'(y)$.
\end{enumerate}

Рассмотрим случай, когда $g(x)$ монотонно возрастает и дифференцируема.

\begin{align*}
  F_Y(y) &= \Pr(Y \le y) = \Pr(g(X) \le y) = \Pr(X \le g^{-1}(y)) = F_X(g^{-1}(y)) \\
  f_Y(y) &= F_Y'(y) = F_X'(g^{-1}(y)) = f_X(g^{-1}(y)) (g^{-1}(y))' = \frac{f_X(g^{-1}(y))}{g'(g^{-1}(y))}
\end{align*}

Аналогично можно рассмотреть случай, когда $g(x)$ монотонно убывает и дифференцируема
\begin{align*}
  F_Y(y) &= \Pr(Y \le y) = \Pr(g(X) \le y) = \Pr(X \ge g^{-1}(y)) = 1 - F_X(g^{-1}(y)) \\
  f_Y(y) &= F_Y'(y) = -F_X'(g^{-1}(y)) = -f_X(g^{-1}(y)) (g^{-1}(y))' = -\frac{f_X(g^{-1}(y))}{g'(g^{-1}(y))} = \frac{f_X(g^{-1}(y))}{|g'(g^{-1}(y))|}
\end{align*}

То есть в общем случае (когда $g(x)$ строго монотонна):
\begin{align*}
  f_Y(y) = \frac{f_X(g^{-1}(y))}{|g'(g^{-1}(y))|}
\end{align*}

Интуитивное объяснение (которое использовалось на практике). Пусть $y = g(x)$

\begin{align*}
  f_Y(y) \delta_y \approx \Pr(y \le Y \le y + \delta_y) = \Pr(x \le X \le x + \delta_y) = f_X(x) \delta_x = f_X(x) \frac{\delta_y}{g'(x)}
\end{align*}

\begin{center}
  \begin{tikzpicture}
      \begin{axis}[xmin=0, xmax=4, ymin=0, ymax=4,
      grid=major,
      ylabel={$Y = g(X)$},
      xlabel={$X$}, smooth]
        \addplot [blue] coordinates {(0, 0) (1, 1.2) (1.5, 1.4) (1.7, 1.5) (2.2, 2) (3, 3) (3.5, 4)};
        \coordinate (x) at (axis cs: 1.5,0);
        \coordinate (dx) at (axis cs:1.7,0);
        \coordinate (y) at (axis cs: 0,1.4);
        \coordinate (dy) at (axis cs:0,1.5);

        \coordinate (xy) at (axis cs:1.5,1.4);
        \coordinate (dxy) at (axis cs:1.7,1.5);

      \end{axis}

      \draw [dashed] (x) -- (xy) -- (y);
      \draw [dashed] (dx) -- (dxy) -- (dy);

      \draw [ultra thick, red] (x) -- (dx) node [pos=0.5, below, black] {$\delta_x$};
      \draw [ultra thick, blue] (y) -- (dy) node (cy) [pos=0.5] {};

      \node (label) at (-2, 0) {$\delta_y = \delta_x g'(x)$};
      \draw [->] (label) -- (cy); 

      \draw [ultra thick] (xy) -- (dxy);
  \end{tikzpicture}
\end{center}

Для немонотонных $g(x)$ алгоритм остается прежний, но нет гарантий, что все пройдет легко. рассмотрим $g(x) = x^2$ и $Y = g(X)$ (при известной плотности $X$).

\begin{align*}
  F_Y(y) &= \Pr(Y \le y) = \Pr(X^2 \le y) = 1 - \Pr(X^2 > y) \\
         &= 1 - \Pr(X > \sqrt{y}) - \Pr(X < -\sqrt{y}) \\
         &= \Pr(X \le \sqrt{y}) = \Pr(X < -\sqrt{y}) = F_X(\sqrt{y}) - F_X(-sqrt{y}) \\
  f_Y(y) &= F_Y'(y) = F_X'(\sqrt{y}) - F_X'(-\sqrt{y}) = f_X(\sqrt{y}) \frac{1}{2\sqrt{y}} - f_X(-\sqrt{y}) \left(-\frac{1}{2\sqrt{y}}\right) \\
         &= \frac{f_X(\sqrt{y}) + f_X(-\sqrt{y}) }{2\sqrt{y}}  
\end{align*}

\section{Функции нескольких с.в.}

Если есть $Z = g(X, Y)$, то алгоритм нахождения ее функции распределения или плотности такой же. Рассмотрим на примере. $X, Y$ -- независимые, обе следуют $U(0, 1)$. Рассмотрим с.в. $Z = \frac{Y}{X}$.

Найдем функцию распределения $F_Z(z)$

\begin{align*}
  F_Z(z) = \Pr(\frac{Y}{X} \le z).
\end{align*}


\begin{center}
  \begin{tikzpicture}
      \begin{axis}[xmin=0, xmax=1.2, ymin=0, ymax=1.2,
      grid=major,
      ylabel={$Y = g(X)$},
      xlabel={$X$}]
        \addplot [draw=none, fill=blue!20] coordinates {(0, 0) (0.5, 1) (1, 1) (1, 0)} \closedcycle;
        \addplot [draw=none, fill=red!20] coordinates {(0, 0) (1, 0.5) (1, 0)} \closedcycle;
        \addplot [black, ultra thick] coordinates {(0, 1) (1, 1) (1, 0)};
        \addplot [red] coordinates {(0, 0) (1.2, 0.6)};
        \addplot [blue] coordinates {(0, 0) (0.6, 1.2)};
      \end{axis}
  \end{tikzpicture}
\end{center}

Если $z$ меньше единицы, то это просто интеграл по красному треугольнику, то есть его площадь (так как совместная плотность $f_{X, Y}(x, y) = 1$ на всем квадрате).

\begin{align*}
  F_Z(z) = \frac{z}{2}, \text{ если } z \in [0, 1].
\end{align*}

Если $z > 1$ , то это площадь всего квадрата минус площадь незакрашенного треугольника

\begin{align*}
  F_Z(z) = 1 - \frac{1}{2z}.
\end{align*}

Объединяя все вместе, получаем:

\begin{align*}
  F_Z(z) = \begin{cases}
    0, &\text{ если } z < 0, \\
    \frac{z}{2}, &\text{ если } z \in [0, 1), \\
    1 - \frac{1}{2z}, &\text{ если } z \ge 1. \\
  \end{cases}
\end{align*}

\subsection{Сумма независимых с.в.}

В данном подразделе мы имеем две независимых с.в. $X$ и $Y$, знаем их распределение и хотим узнать распределение с.в. $Z = X + Y$. 

\textbf{Дискретный случай}

Чтобы посчитать вероятность того, что $Z = z$, необходимо, чтобы одновременно выполнялось:
\begin{itemize}
  \item $X = x$ (где $x$ --- какое-то число из множества значений $X$)
  \item $Y = z - x$.
\end{itemize}

То есть мы берем все пары $(x, y)$ с прямой $x + y = z$ и суммируем вероятности этих пар.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[xmin=-1, xmax=3, ymin=-1, ymax=3,
      grid=major,
      ylabel={$y$},
      xlabel={$x$}]
      \addplot coordinates {(-2, 4) (-0.5, 2.5) (0, 2) (0.5, 1.5) (1, 1) (2, 0) (4, -2)};
      \coordinate (a1) at (axis cs:-0.5, 2.5);
      \coordinate (a2) at (axis cs:0, 2);
      \coordinate (a3) at (axis cs:0.5, 1.5);
      \coordinate (a4) at (axis cs:1, 1);
      \coordinate (a5) at (axis cs:2, 0);
    \end{axis}

    \node [above right] at (a1) {$(-0.5, 2.5)$};
    \node [above right] at (a2) {$(0, 2)$};
    \node [above right] at (a3) {$(0.5, 1.5)$};
    \node [above right] at (a4) {$(1, 1)$};
    \node [above right] at (a5) {$(2, 0)$};

    
  \end{tikzpicture}
\end{center}

Заметим, что из-за независимости $X$ и $Y$ события $X = x$ и $Y = z - x$ независимы, поэтому 
\[
  \Pr(X = x \cap Y = z - x) = p_X(x) p_Y(z - x). 
\]
Таким образом, формула выглядит так:

\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.4\textwidth}
      \[
        p_{Z}(z) = \sum_{x} p_X(x) p_Y(z - x)  
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

\textbf{Непрерывный случай}

Аналогичная формула для непрерывных с.в.:
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.4\textwidth}
      \[
        f_{Z}(z) = \int_{-\infty}^{+\infty} f_X(x) f_Y(z - x) dx  
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}
Чтобы ее доказать, обусловимся сначала на событии $X = x$. Тогда 
\begin{align*}
  f_{Z \mid X}(z \mid x) = f_{Y + x \mid X}(z \mid x) = f_{Y + x}(z) = f_Y(z - x). 
\end{align*}
Далее, по формуле полной вероятности имеем
\begin{align*}
  f_Z(z) = \int_{-\infty}^{+\infty} f_X(x) f_{Z \mid X} (z \mid x) dx = \int_{-\infty}^{+\infty} f_X(x) f_Y(z - x) dx/
\end{align*}

\textbf{Интересное следствие}

Сумма независимых нормальных распределений есть нормальное распределение. Пусть $X \sim N(\mu_X, \sigma_X^2)$ и $Y \sim N(\mu_Y, \sigma_Y^2)$ --- независимы. Посчитаем $f_Z(z)$, где $Z = X + Y$.

\begin{align*}
  f_Z(z) &= \int_{-\infty}^{+\infty} f_X(x) f_Y(z - x) dx \\
         &= \int_{-\infty}^{+\infty} \frac{1}{\sigma_X \sqrt{2\pi}} \exp\left(-\frac{(x - \mu_X)^2}{2\sigma_X^2}\right) \cdot \frac{1}{\sigma_Y \sqrt{2\pi}} \exp\left(-\frac{(z - x - \mu_Y)^2}{2\sigma_Y^2}\right)dx \\
         &= \frac{1}{\sqrt{2\pi(\sigma_X^2 + \sigma_Y^2)}} \exp\left(-\frac{(z - \mu_X - \mu_Y)^2}{2(\sigma_X^2 + \sigma_Y^2)}\right)
\end{align*}
Тут было много несложной, но громоздкой математики, которую мы опустили. Таким образом, $Z \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$. Единственное удивительное здесь -- то, что мы по-прежнему имеем нормальное распределение. Его параметры при этом очевидны из линейности матожиданий и из свойств дисперсии независимых с.в. 

По индукции легко доказать, что для любого конечного множества независимых нормальных с.в. их сумма будет также нормальной с.в.


\section{Эмитация одного распределения другим}

Практически любое современное вычислительное устройство умеет генерировать (псевдо)случайные числа, но чаще всего только равномерное распределение. Этого на самом деле достаточно, чтобы получить случайную величину, следующую любому другому распределению. В данной части мы предполагаем, что мы умеем получать с.в. $X \sim U(0 ,1)$ и хотим получить с.в. $Y$, для которой знаем функцию распределения $F_Y(y)$.

Рассмотрим простой случай. $Y$ -- дискретная с.в. с конечным набором значений. Пусть ее функция вероятности выглядит так:

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[ybar, ymin=0,
      grid=major,
      ytick = {0.35, 0.2, 0.45},
      ylabel={$p_{Y}(y)$},
      xlabel={$y$}]
      \addplot
      [draw=blue, fill=blue] 
      coordinates
        {(0, 0.35) (2,0.2) (3,0.45)};
    \end{axis}
  \end{tikzpicture}
\end{center}

Как мы можем поступить. Возьмем отрезок единичной длины, разобъем его на отрезки длиной, соответствующей вероятности каждого значения с.в., сгенерим $X$ и вернем значение, соответствующее тому значению, в которое попал $X$ .

\begin{center}
  \begin{tikzpicture}
    \draw [ultra thick] (0, 0) rectangle (10, 1);

    \draw (3.5, -0.3) -- (3.5, 1);
    \draw (5.5, -0.3) -- (5.5, 1);
    \draw (0, 0) -- (0, -0.3);
    \draw (10, 0) -- (10, -0.3);

    \node at (1.75, 0.5) {$0$};
    \node at (4.5, 0.5) {$2$};
    \node at (7.75, 0.5) {$3$};

    \draw [<->] (0, -0.3) -- (3.5, -0.3) node [pos=0.5, below] {$0.35$};
    \draw [<->] (3.5, -0.3) -- (5.5, -0.3) node [pos=0.5, below] {$0.2$};
    \draw [<->] (5.5, -0.3) -- (10, -0.3) node [pos=0.5, below] {$0.45$};
  \end{tikzpicture}
\end{center}

Таким методом можно сэмитировать любую с.в. с конечным множеством значений размера $n$ за время запроса к $X \sim U(0, 1)$ плюс время $\Theta(\log(n))$, необходимое для нахождения отрезка, в который мы попали, двоичным поиском.

Интерпретация наших действий на основе функции распределения:

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
      grid=major,
      xmin = -1,
      xmax = 4,
      ytick = {0, 0.35, 0.55, 1},
      ylabel={$F_{Y}(y)$},
      xlabel={$y$}]
      \addplot
      [draw=blue, ultra thick] 
      coordinates
        {(-1, 0) (0,0) (0, 0.35) (2, 0.35) (2, 0.55) (3, 0.55) (3, 1) (4, 1)};

      \coordinate (y) at (axis cs:-1, 0.75);
      \coordinate (x) at (axis cs:3, 0);
      \coordinate (xy) at (axis cs:3, 0.75);
    \end{axis}

    \draw [red, dashed, ultra thick] (y) -- (xy) -- (x);
    \node [left] at (y) {генерим $X$ здесь};
    \node [below=1cm] at (x) {берем $Y$ отсюда};
  \end{tikzpicture}
\end{center}

Давайте используем эту интерпретацию для непрерывной $Y$ в случае с монотонной $F_Y(y)$.

\begin{center}
  \begin{tikzpicture}
    \begin{axis}[
      grid=major,
      xmin = -1,
      xmax = 4,
      ylabel={$F_{Y}(y)$},
      xlabel={$y$},
      smooth]
      \addplot
      [draw=blue, ultra thick] 
      coordinates
        {(-1, 0) (0, 0.1) (1, 0.5) (2, 0.75) (3, 0.9) (4, 1)};

      \coordinate (y) at (axis cs:-1, 0.75);
      \coordinate (x) at (axis cs:2, 0);
      \coordinate (xy) at (axis cs:2, 0.75);
    \end{axis}

    \draw [red, dashed, ultra thick] (y) -- (xy) -- (x);
    \node [left=0.5cm] at (y) {генерим $X$ здесь};
    \node [below=1.5cm] at (x) {берем $Y$ отсюда};
  \end{tikzpicture}
\end{center}

То есть после выбора $X$ мы берем $Y = F_Y^{-1}(X)$. Почему это работает:

\begin{align*}
  \Pr(Y \le y) = \Pr(F_Y^{-1}(X) \le y) = \Pr(X \le F_Y(y)) = F_Y(y)
\end{align*}

\section{Ковариация}

До сих пор мы про две с.в. могли сказать только то, зависимы ли они, или нет. Но иногда хочется определить степень этой зависимости, то есть понять, сколько информации одна с.в. дает про другую с.в. Рассмотрим пример. Пусть две с.в. $X$ и $Y$ имеют нулевое матожидание. Если они независимы, то $E[XY] = E[X]E[Y] = 0$. Но рассмотрим такие случаи: пусть каждая точка на графике равновероятна.

\begin{center}
  \begin{tikzpicture}% coordinates%\begin{}[]%\addplot coordinates {};%\end{}%\end{}
    \begin{axis}[
      grid=major,
      xmin = -5,
      xmax = 5,
      ymin=-4,
      ymax=4,
      ylabel={$Y$},
      xlabel={$X$},
      only marks]
      \addplot coordinates {(1.79, 1.10)
      (-0.31, 0.02)
      (-1.80, -0.54)
      (2.62, 1.24)
      (1.11, 0.40)
      (-4.07, -2.07)
      (0.21, 0.58)
      (-0.95, -0.29)
      (-1.36, -0.14)
      (4.68, 2.33)
      (-0.56, 0.20)
      (-1.11, -0.80)
      (-1.03, -1.39)
      (4.22, 2.11)
      (-0.24, 0.46)
      (-1.97, -1.52)
      (3.05, 1.36)
      (1.07, 0.35)
      (-0.70, -0.96)
      (-0.64, -0.04)
      (-2.49, -1.73)
      (0.89, 1.17)
      (-3.79, -1.42)
      (2.73, 1.41)
      (0.41, 0.24)
      (0.76, -0.16)
      (1.19, 1.62)
      (-1.76, -1.41)
      (0.85, 0.49)
      (1.14, -0.12)
      (-2.98, -0.84)
      (-2.33, -0.10)
      (-3.05, -1.98)
      (-1.40, -0.55)
      (1.89, 0.25)
      (-0.01, 0.33)
      (2.71, 1.09)
      (-3.76, -2.04)
      (-0.66, 0.03)
      (1.33, 1.14)
      (-1.91, -0.78)
      (-1.39, -0.16)
      (-3.60, -2.14)
      (0.57, 1.29)
      (4.37, 2.35)
      (-3.49, -2.17)
      (2.93, 1.23)
      (-4.45, -3.20)
      (1.77, 1.19)
      (-0.36, 0.51)
      (1.09, 0.78)
      (-3.34, -0.64)
      (0.39, 1.43)
      (4.79, 1.53)
      (2.70, 1.19)
      (1.23, 0.56)
      (4.91, 2.33)
      (1.73, 1.04)
      (2.10, 1.67)
      (1.86, 0.31)
      (-0.97, -1.34)
      (-2.53, -1.36)
      (1.21, 0.38)
      (1.53, 1.03)
      (3.07, 1.19)
      (0.53, 0.34)
      (-3.30, -1.89)
      (0.24, 0.33)
      (-2.62, -1.66)
      (1.86, 2.48)
      (2.74, 2.57)
      (1.74, 0.46)
      (-2.36, -1.27)
      (0.95, 0.32)
      (2.16, 0.88)
      (-4.73, -2.15)
      (-0.29, -0.52)
      (-0.82, 0.87)
      (3.12, 2.32)
      (1.59, 1.63)
      (-2.63, -2.17)
      (-4.67, -1.73)
      (2.74, 0.99)
      (0.66, 0.28)
      (3.26, 1.51)
      (2.10, 0.50)
      (2.84, 0.23)
      (-2.61, -1.30)
      (-1.63, 0.16)
      (-2.46, -0.86)};
    \end{axis}
  \end{tikzpicture}
\end{center}

В таком случае велика вероятность, что либо $X$ и $Y$ оба меньше нуля, либо оба больше нуля, то есть $E[XY] > 0$. В следующем случае будет наоборот: $X$ и $Y$ с большой вероятностью имеют разные знаки, значит, $E[XY] < 0$.

\begin{center}
  \begin{tikzpicture}% coordinates%\begin{}[]%\addplot coordinates {};%\end{}%\end{}
    \begin{axis}[
      grid=major,
      xmin = -5,
      xmax = 5,
      ymin=-4,
      ymax=4,
      ylabel={$Y$},
      xlabel={$X$},
      only marks]
      \addplot coordinates {(1.65, -3.79)
      (-0.26, 1.20)
      (-1.22, 3.23)
      (-0.35, 0.07)
      (-0.06, 0.90)
      (-1.68, 3.60)
      (0.87, 0.90)
      (0.50, -0.59)
      (0.99, -1.34)
      (-0.98, 1.31)
      (0.97, -1.53)
      (-0.19, -1.49)
      (-0.95, 0.37)
      (0.41, -1.27)
      (-0.65, 1.24)
      (-0.15, -0.31)
      (0.71, -2.19)
      (-0.84, 0.75)
      (-0.85, 0.65)
      (-0.49, 1.43)
      (1.80, -2.17)
      (0.58, -1.11)
      (-0.93, 2.21)
      (-0.49, -2.35)
      (0.10, 0.31)
      (-0.54, 1.94)
      (-0.32, -0.22)
      (-1.68, 2.34)
      (1.36, -0.67)
      (-0.13, -0.36)
      (0.22, 2.14)
      (-1.21, 3.05)
      (0.63, -1.31)
      (-1.18, 0.91)
      (1.45, -3.75)
      (-0.30, -1.11)
      (0.61, -1.66)
      (-0.88, 0.23)
      (-0.30, 1.03)
      (-0.74, 2.16)
      (0.55, -0.87)
      (-0.10, 0.98)
      (1.06, -0.77)
      (-1.07, 3.94)
      (-0.10, -1.49)
      (-0.44, 1.63)
      (-0.05, -0.90)
      (1.33, -2.17)
      (0.04, -0.77)
      (-0.92, 1.97)
      (-1.09, 1.73)
      (-1.20, 2.78)
      (-0.63, 1.46)
      (1.32, -3.63)
      (1.58, -2.74)
      (0.01, 0.07)
      (-1.37, 1.56)
      (1.26, -3.13)
      (1.66, -2.43)
      (0.65, -1.80)
      (1.03, -2.17)};
    \end{axis}
  \end{tikzpicture}
\end{center}

Чтобы измерить степень зависимости была введена величина, называемая \emph{ковариацией} случайных величин $X$ и $Y$.

\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \Cov(X, Y) = E\bigg[(X - E[X])(Y - E[Y])\bigg]
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

Если две с.в. независимы, то для них будет верно
\begin{align*}
  \Cov(X, Y) = E(X - E(X)) E(Y - E(Y)) = 0
  \cdot 0 = 0
\end{align*} 

Правда обратное неверно: с.в. с нулевой ковариацией могут быть зависимы. Рассмотрим пример. Пусть четыре пары $(X, Y)$ равновероятны: $(1, 0), (0, 1), (-1, 0), (0, -1)$

\begin{center}
  \begin{tikzpicture}% coordinates%\begin{}[]%\addplot coordinates {};%\end{}%\end{}
    \begin{axis}[
      grid=major,
      xmin = -1.2,
      xmax = 1.2,
      ymin=-1.2,
      ymax=1.2,
      ylabel={$Y$},
      xlabel={$X$},
      only marks]
      \addplot coordinates {(1, 0) (0, 1) (-1, 0) (0, -1)};
    \end{axis}
  \end{tikzpicture}
\end{center}

Заметим, что матожидания $X$ и $Y$ равны нулю, то есть $\Cov(X, Y) = E[XY]$, но $XY$ всегда равен нулю, поэтому ковариация нулевая. При этом $X$ и $Y$ явно зависимы: если $X = 1$, то $Y$ однозначно равен нулю.

\section{Свойства ковариации}

\textbf{Полезная формула.}

Ковариация с.в. с самой собой равна ее дисперсии, причем у нас была очень милая формула:
\begin{align*}
  \Cov(X, X) = E[(X -E[X])^2] = E[X^2] - (E[X])^2.
\end{align*}

Есть ли что-то подобное для ковариации? Конечно, есть.

\begin{align*}
  \Cov(X, Y) &= E\bigg[(X - E[X])(Y - E[Y])\bigg] = E\bigg[XY - YE[X] - XE[Y] + E[X]E[Y]\bigg] \\
  &= E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] = E[XY] - E[X]E[Y].
\end{align*}

\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \Cov(X, Y) = E[XY] - E[X]E[Y]
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

\textbf{Линейные преобразования}

Линейное преобразование одной с.в.:

\begin{align*}
  \Cov(aX + b, Y) &= E[(aX + b)Y] - E[aX + b]E[Y] \\
                  &= aE[XY] +bE[Y] - aE[X]E[Y] - bE[Y]\\
                  &= a(E[XY] - E[X]E[Y]) = a \Cov(X, Y) 
\end{align*}
Заметим, что прибавление константы к одной с.в. ничего не меняет, так как не меняется распределение $X - E[X]$, а умножение на константу как раз увеличивает этот множитель в ту же константу.


Одна с.в. -- cумма двух других:
\begin{align*}
  \Cov(X + Y, Z) &= E\bigg[(X + Y - E[X + Y])(Z - E[Z])\bigg] \\
                 &= E\bigg[(X - E[X])(Z - E[Z])\bigg] + E\bigg[(Y - E[Y])(Z - E[Z]) \bigg] \\
                 &= \Cov(X, Y) + \Cov(Y, Z)
\end{align*}


\textbf{Ковариация и дисперсия суммы с.в.}

Распишем дисперсию суммы двух с.в. по формуле

\begin{align*}
  \Var(X + Y) &= E\bigg[(X + Y - E[X + Y])^2\bigg] \\ 
              &= E\bigg[((X - E[X]) + (Y - E[Y]))^2\bigg] \\
              &= E[(X - E[X])^2] + E[(Y - E[Y])^2] + 2E\bigg[(X - E[X]) (Y - E[Y])\bigg] \\
              &= \Var(X) + \Var(Y) + 2\Cov(X, Y)
\end{align*}

Что насчет более двух с.в.? Предположим, что у нас есть набор с.в. $X_1, \dots, X_n$, и матожидание каждой с.в. равно нулю. Тогда имеем

\begin{align*}
  \Var(X_1 + \dots +X_n) &= E((X_1 + \dots + X_n)^2) \\
                         &= E\left(\sum_{i = 1}^n X_i^2 + \sum_{i \ne j} X_i X_j \right) \\
                         &= \sum_{i = 1}^n \Var(X_i) + \sum_{i \ne j} \Cov(X_i, X_j)
\end{align*}

Если хотим показать то же самое для с.в. с ненулевым ожидаением, то заметим, что добавление константы к с.в. не меняет ни ее дисперсию, ни ковариацию с другой с.в., поэтому


\begin{align*}
  \Var(X_1 + \dots +X_n) &= \Var((X_1 - E[X_1]) + \dots + (X_n - E[X_n])) \\
                         &= \sum_{i = 1}^n \Var(X_i - E[X_i]) + \sum_{i \ne j} \Cov((X_i - E[X_i]), (X_j - E[X_j])) \\
                         &= \sum_{i = 1}^n \Var(X_i) + \sum_{i \ne j} \Cov(X_i, X_j)
\end{align*}

\section{Коэффициент корреляции}

Ковариация -- очень странная величина. Ее размер по сути зависит от разброса двух с.в., котороые являются ее аргументом. То есть по фразе ``ковариация $X$ и $Y$ равна единице'' очень сложно сказать, много это, или мало, так как мы не знаем масштаба $X$ и $Y$. Например, когда мы пытаемся посчитать ковариацию температуры воздуха и давления, то в зависимости от используемых величин измерения (цельсий или фаренгейты, паскали или атмосферы) будет разная ковариация. Причем у самой ковариации всегда есть единица измерения, равная единице измерения $X$, умноженная на $Y$ (например, градус цельсия * паскаль), что еще больше усложняет осознание степени зависимости пары с.в. Поэтому была введена мера зависимости с.в., называемая коэффициентом корреляции $\rho(X, Y)$.

\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \rho(X, Y) = \frac{\Cov(X, Y)}{\sigma_X \sigma_Y}
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

\emph{NB:} он определен только для с.в. с ненулевой дисперсией. 

Его основные свойсва:

\begin{enumerate}
  \item $\rho(X, Y)\in [-1, 1]$
  \item Если $X$ и $Y$ независимы, то $\rho(X, Y) = 0$, как и ковариация (обратное не всегда верно, см. пример для нулевой ковариации)
  \item Если $|\rho(X, Y)| = 1$, то $X$ и $Y$ линейно зависимы, то есть $(X - E[X]) = c(Y - E[Y])$. Как следствие: $\rho(X, X) = \frac{\Cov(X, X)}{\sigma_X^2} = 1$.
  \item Линейные преобразования любой из двух с.в. не меняют модуль коэффициента корреляции: $\rho(aX + b, Y) = \frac{a\Cov(X, Y)}{|a|\sigma_X \sigma_Y} = \sign(a)\rho(X, Y)$
\end{enumerate}

Для доказательства первого и третьего свойств рассмотрим с.в. $X$ и $Y$ с нулевыми матожиданиями и единичной дисперсией (для общего случая легко доказать все, перейдя к с.в. $X' = \frac{X - E[X]}{\sigma_X}$ и $Y' = \frac{Y - E[Y]}{\sigma_Y}$, но мы опустим это для упрощения вычислений). Рассмотрим с.в. $Z = (X - \rho(X, Y)Y)^2$. Она неотрицательна, значит, и ее матожидание неотрицательно.

\begin{align*}
  E[Z] &= E[(X - \rho Y)^2] = E[X^2] - 2\rho E[XY] + \rho^2 E[Y^2] \\
       &= \Var(X) - 2 \rho \cdot \rho + \rho^2 \Var(Y) = 1 - \rho^2 \ge 0   
\end{align*}

Отсюда следует, что во-первых, $\rho^2 \le 1$, а во-вторых, если $|\rho| = 1$, то $E[Z] = 0$, что возможно только если $Z$ всегда равен нулю, что в свою очередь подразумевает, что $X$ всегда равен $\pm Y$ (в зависимости от знака $\rho$).

\section{Интерпретация корреляции}

Если две случайных величины зависимы, то чаще всего это не значит, что одна определяет другую и наоборот. Например, проведя исследование оценок в школе, можно обнаружить, что у учеников с хорошей оценкой по математике также хорошие оценки по литературе, однако это не значит, что знания по математике помогают в изучении литературы и наоборот. Это скорее означает, что у двух этих случайных величин есть какой-то общий фактор, который влияет на них обеих. В случае с оценками это может быть, например, степень усердия, которое тот или иной ученик прилагает к учебе. 

Чуть более формально, давайте представим, что есть три случайных величины $Z, U$ и $V$ c нулевыми матожиданиями и единичными дисперсиями, которые все независимы друг от друга. И рассмотрим две других с.в. $X = Z + U$ и $Y = Z + V$. Посчитаем $\rho(X, Y)$

\begin{align*}
  \Var(X) &= \Var(Y) = \Var(Z) + \Var(U) = 2 \\
  \sigma_X &= \sigma_Y = \sqrt(2) \\
  E[XY] &= E[Z^2 + UZ + VZ + UV] = \Var(Z) = 1 \\
  \rho(X, Y) &= \frac{E[XY]}{\sigma_X \sigma_Y} = \frac{1}{2}
\end{align*}

То есть коэффициент корреляции определяет вклад какой-то причины, которая влияет на обе с.в., в каждую с.в.

\section{Пример важности корреляции}

Допустим, вы решили инвестировать. И у вас есть 10\$, и вы вкладываете по доллару в какие-то 10 компаний. Средняя выручка с каждой компании равна 1\$ и среднеквадратичное отклонение выручки с каждой компании есть $1.3$\$. Очевидно, ваша суммарная выручка равна вашему вкладу, но каково среднеквадратичное отклонение? Если выручки от разных компаний независимы, То
\begin{align*}
  \Var(X_1 + \dots + X_10) &= \sum_{i = 1}^{10} \Var(X_i) = 10 \cdot 1.3^2 = 16.9 \\
  \sigma_{X_1 + \dots + X_10} &= \sqrt{16.9} \approx 4.1
\end{align*}
То есть сумма довольно хорошо сконцентрирована, больше, чем при вкладе всего лишь в одну компанию. На большую прибыль надеяться не стоит, но и много проиграть шансов не так много. Теперь допустим, что у всех выручек есть какой-то общий фактор, который под ними лежит, и коэффициент корелляции между любыми $X_i$ и $X_j$ равен 0.9. Тогда
\begin{align*}
  \Var(X_1 + \dots + X_10) &= \sum_{i = 1}^{10} \Var(X_i) - \sum_{i \ne j} \Cov(X_i, X_j) = 10 \cdot 1.3^2  + 90 \cdot 0.9 \cdot (1.3)^2 \approx 154 \\
  \sigma_{X_1 + \dots + X_10} &= \sqrt{16.9} \approx 12.4
\end{align*}
То есть если падают акции хотя бы одной компании, то с большой вероятностью падают и акции других, что приводит к большим потерям. Примерно это и случилось в 2008 году, когда рухнул весь рынок сразу.



\end{document}