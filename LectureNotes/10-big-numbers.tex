\documentclass[12pt]{article}

\usepackage{a4wide}

\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,patchplots}
\usetikzlibrary{decorations.pathreplacing,calc,tikzmark, patterns,arrows.meta}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}
\pgfmathdeclarefunction{gauss2d}{6}{%
  \pgfmathparse{1/(#3*#6*2*pi)*exp(-((#1-#2)^2)/(2*#3^2) - ((#4-#5)^2)/(2*#6^2))}%
}

\usepackage{xspace}

\usepackage{mathtools}
\usepackage{cite}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}

\newcommand\N{\mathbb{N}}
\newcommand\R{\mathbb{R}}
\newcommand\eps{\varepsilon}
\newcommand\one{\mathbbm{1}}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\pow}{pow}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}{Лемма}

\title{Лекция 10. Границы Чернова, сходимость и законы больших чисел.}

\begin{document}
\maketitle

\section{Границы Чернова}



Следующие неравенства чаще всего имеются в виду под границами Чернова. Это так называемые "хвостовые оценки" на отклонение суммы случайных величин от ее матожидания.

\begin{theorem}[Хёфдинг]
  \label{thm:chernoff-upper}
 Пусть $X_1, \dots, X_n$ --- независимые с.в., причем для всех $i$ верно $\Pr[X_i \in [0, 1]] = 1$. Пусть $X = \sum_{i = 1}^n X_i$ и $\mu = E[X]$. Тогда для любого $\delta \in (0, \frac{n - \mu}{\mu})$ выполняются следующие неравенства.
  \begin{align*}
    \Pr(X \ge (1 + \delta)\mu) &\le \left(\frac{1}{1 + \delta}\right)^{(1 + \delta)\mu} \left(\frac{n - \mu}{n - (1 + \delta)\mu}\right)^{n - (1 + \delta)\mu} \\
    &\le \left(\frac{e^\delta}{(1 + \delta)^{1 +\delta}}\right)^{\mu} = \exp\left(-((1 + \delta)\ln(1 + \delta) - \delta)\mu\right)\\
    &\le \exp\left(-\frac{\delta^2 \mu}{2 + \frac{2}{3}\delta}\right)\\
    &\le \exp\left(-\frac{\min\{\delta, \delta^2\}\mu}{3}\right).
  \end{align*}  
  Для $\delta = \frac{n - \mu}{\mu}$ левая часть есть $\Pr(X \ge n) = \prod_{i = 1}^n \Pr(X_i = 1)$, а для еще больших значений $\delta$ левая часть равна нулю. 
\end{theorem}

Последние утверждения для $\delta \ge \frac{n - \mu}{\mu}$ очевидны, поэтому докажем неравенства для $\delta < \frac{n - \mu}{\mu}$ (заметьте, что это подразумевает, что $(1 + \delta)\mu < n)$). Сначала докажем первое неравенство, а потом кратко пробежимся по доказательству остальных. Предварительно докажем следующую лемму.

\begin{lemma}
  \label{lem:convex-exp}
  Пусть $X$ --- с.в., такая, что $\Pr[X \in [0, 1]] = 1$. Тогда для любого $t \in \R$
  \begin{align*}
    E[e^{tX}] \le 1 + (e^t - 1)E[X].
  \end{align*}
\end{lemma}
\begin{proof}
  Так как экспонента --- выпуклая вниз функция, то любое для любого $x \in [0, 1]$ верно неравенство:
  \begin{align*}
    e^{tx} \le e^{0t} + \frac{e^{1t} - e^{0t}}{1 - 0} x = 1 + (e^t - 1)x.
  \end{align*} 
  Подставим вместо $x$ с.в. $X$ и возьмем матожидание от обеих частей, неравенство останется верным, что и токазывает утверждение Леммы.
\end{proof}

Теперь мы готовы доказать верхнюю границу Чернова.

\begin{proof}[Доказательство Теоремы~\ref{thm:chernoff-upper}]
  Для всех $i$ обозначим $\mu_i = E[X_i]$. Для любого $t > 0$ верно, что
  \begin{align*}
    \Pr[X \ge (1 + \delta)\mu] &\le \frac{E[e^{tX}]}{e^{(1 + \delta)\mu t}} \text{ (по обобщенным границам Чернова)} \\
                               &= e^{-(1 + \delta)\mu t} \prod_{i = 1}^n E[e^{tX_i}] \text{ (по независимости)} \\
                               &\le e^{-(1 + \delta)\mu t} \prod_{i = 1}^n (1 + (e^t - 1)\mu_i) \text{ (по Лемме~\ref{lem:convex-exp})} \\
                               &\le e^{-(1 + \delta)\mu t} \left(\frac{n + (e^t - 1)\mu}{n}\right)^n \text{ (среднее геом. $\le$ среднее арифм.)} \\
                               &= e^{-(1 + \delta)\mu t} \left(1 + (e^t - 1)\frac{\mu}{n}\right)^n.
  \end{align*}

  Теперь мы хотим минимизировать это выражение, выбрав наилучший $t \in (0, +\infty)$. Для этого продифференцируем его по $t$.
  
  \begin{align*}
    \left(e^{-(1 + \delta)\mu t} \left(1 + (e^t - 1)\frac{\mu}{n}\right)^n\right)' &= -(1 + \delta)\mu e^{-(1 + \delta)\mu t} \left(1 + (e^t - 1)\frac{\mu}{n}\right)^n \\
    &+ e^{-(1 + \delta)\mu t} n \left(1 + (e^t - 1)\frac{\mu}{n}\right)^{n - 1} \frac{\mu}{n} e^t \\
    &= \left(-(1 + \delta)(\left(1 + (e^t - 1)\frac{\mu}{n}\right)) + e^t\right) \cdot \mu e^{-(1 + \delta)\mu t} \left(1 + (e^t - 1)\frac{\mu}{n}\right)^{n - 1}.
  \end{align*}

  Приравнивая это нулю и решая уравнение относительно $e^t$ получаем, что единственный ноль производной в 
  \begin{align*}
    e^{t_0} = \frac{(1 + \delta)(n - \mu)}{n - (1 + \delta) \mu},
  \end{align*}
  причем при меньших значениях $t$ производная отрицательная, а при больших --- положительная. Значит, это точка минимума. Подставляя это значение в наше неравенство, получаем
  \begin{align*}
    \Pr[X \ge (1 + \delta)\mu] &\le \left(\frac{(1 + \delta)(n - \mu)}{n - (1 + \delta) \mu}\right)^{-(1 + \delta)\mu} \cdot \left(1 + \left(\frac{(1 + \delta)(n - \mu)}{n - (1 + \delta) \mu} - 1\right)\frac{\mu}{n}\right)^n \\
    &= \left(\frac{1}{1 + \delta}\right)^{(1 + \delta)\mu} \left(\frac{n - \mu}{n - (1 + \delta)\mu}\right)^{n - (1 + \delta)\mu}. 
  \end{align*}

 Второе неравенство получается за счет наблюдения
 \begin{align*}
  \left(\frac{n - \mu}{n - (1 + \delta)\mu}\right)^{n - (1 + \delta)\mu} = \left(1 + \frac{\delta\mu}{n - (1 + \delta)\mu}\right)^{\frac{n - (1 + \delta)\mu}{\delta\mu} \cdot \delta \mu} \le e^{\delta\mu}.
 \end{align*}
  Следующее равенство получается путем логарифмирования: $x = e^{\ln(x)}$. Следующее неравенство -- через неравенство, получаемое аккуратным анализом двух фукнций:
  \begin{align*}
    (1 + \delta)\ln(1 + \delta) - \delta \ge \frac{\delta^2}{2 + \frac{2}{3}\delta},
  \end{align*}
  и последнее неравенство --- аккуратным рассмотрением двух случаев: $\delta \ge 1$ и $\delta < 1$. Мы опускаем все эти подробности, так как к теорверу они не имеют особого отношения, это больше про матан.
\end{proof}

Также есть аналогичные нижние границы Чернова, которые выглядят так (в том же сеттинге, что и верхние, только $\delta \in (0, 1)$).

\begin{align*}
  \Pr(X \le (1 - \delta)\mu) &\le \left(\frac{1}{1 - \delta}\right)^{(1 - \delta)\mu} \left(\frac{n - \mu}{n - (1 - \delta)\mu}\right)^{n - (1 - \delta)\mu} \\
  &\le \left(\frac{e^\delta}{(1 - \delta)^{1 - \delta}}\right)^{\mu} \\
  &\le \exp\left(-\frac{\delta^2\mu}{2}\right).
\end{align*}

Первое неравенство доказывается тем, что мы рассматриваем $Y_i = 1 - X_i$ и $Y = n - X$. Тогда
\begin{align*}
  \Pr(X \le (1 - \delta)\mu) &= \Pr((n - X) \ge n - (1 - \delta)\mu)  = \Pr\left(Y \ge (n - \mu)\left(1 + \delta \frac{\mu}{n - \mu}\right) \right) \\
  &\le \left(\frac{1}{1 + \delta \frac{\mu}{n - \mu}}\right)^{(1 + \delta \frac{\mu}{n - \mu})(n - \mu)} \left(\frac{n - (n - \mu)}{n - \left(1 + \delta \frac{\mu}{n - \mu}\right)(n - \mu)}\right)^{n - (1 + \delta \frac{\mu}{n - \mu})(n - \mu)} \\
  &= \left(\frac{n - \mu}{n - (1 - \delta)\mu}\right)^{n - (1 - \delta)\mu} \left(\frac{1}{1 - \delta}\right)^{(1 - \delta)\mu}.
\end{align*}

Последняя форма границ Чернова, которую мы рассмотрим --- с использованием дисперсии. Пусть у нас есть независимые $X_1, \dots, X_n$, причем все они не превосходят свое матожидание более, чем на 1 с вероятностью 1 (то есть $\Pr(X_i \le E[X_i] + 1) = 1$). Пусть $X$ --- их сумма, а $\sigma^2 = \Var(X)$. Тогда для любой $\lambda > 0$

\begin{align*}
  \Pr(X \ge E[X] + \lambda) &\le \left(\left(1 + \frac{\lambda}{\sigma^2}\right)^{-(1 + \frac{\lambda}{\sigma^2})\frac{\sigma^2}{n + \sigma^2}} \left(1 - \frac{\lambda}{n}\right)^{-(1 - \frac{\lambda}{n})\frac{n}{n + \sigma^2}} \right)^n \\
  &\le \exp\left(-\lambda\left(\left(1 + \frac{\sigma^2}{\lambda}\right)\ln\left(1 + \frac{\lambda}{\sigma^2}\right) - 1\right)\right) \\
  &\le \exp\left(-\frac{\lambda^2}{2\sigma^2 + \frac{2}{3}\lambda}\right) \\
  &\le \exp\left(-\frac{1}{3} \min\left\{ \frac{\lambda^2}{\delta^2}, \lambda\right\}\right).
\end{align*}

\section{Еще пара неравенств}

\textbf{Неравенство Беннета}

Пусть $X_1, \dots X_n$ --- независимые с.в. с нулевыми матожиданиями ($\forall i\in[1..n] E[X_i] = 0$) и каждый $X_i$ с вероятностью 1 не превосходит какое-то $a$. Пусть $X = \sum_{i = 1}^n X_i$ и $\sigma = \sqrt{\Var(X)}$. Тогда для всех $t > 0$

\begin{align*}
  \Pr(X > t) \le \exp\left(-\frac{\sigma^2}{a^2} h\left(\frac{at}{\sigma^2}\right)\right),
\end{align*}
где $h(x) = (1 + x)\ln(1 + x) - x$ --- возрастающая по $x$ функция.

\textbf{Неравенство Бернштейна}

Те же условия, только требуем $|X_i| \le a$ с вероятностью 1. Тогда 
\begin{align*}
  \Pr(X > t) \le \exp\left(\frac{t^2 / 2}{\sigma^2 + at/3}\right).
\end{align*}

Очевидно, оба эти неравенства можно обобщить на $X_i$ с ненулевыми ожиданиями и с разными границами путем линейных преобразований и приведению всех $X_i$ к условиям неравенств. В таком случае оба неравенства Бернштейна сравнимы с неравенством Хёфдинга, но могут давать лучшие границы при высокой концентрации с.в. $X_i$ (когда $\sigma$ получается меньше, чем сумма длин допустимых интервалов для всех $X_i$).

\section{Сходимость в вероятностных пространствах.}

В этом разделе мы будем рассматривать бесконечные последовательности с.в. $\{X_n\}_{n \in \N}$ и определим, что такое сходимость такой последовательности, пытаясь создать аналогию с пределом числовой последовательности.

\textbf{Сходимость по распределению.}

Говорят, что $X_n \to X$ по распределению (все $X_n$ и $X$ -- с.в. на одном и том же вероятностном пространстве), если функции распределения $F_{X_i}(x)$ поточечно сходятся к функции распределения $F_X(x)$, кроме, возможно, счетного числа точек разрыва $F_X(x)$. Последнее уточнение важно, так как возьмем, например, последовательность $X_n \sim U(0, \frac{1}{n})$. В пределе, казалось бы, они дадут с.в. $X = 0$. Но все $F_{X_n}(0) = 0$, в то время как $F_X(0) = 1$.

Также заметим, что сходимость по распределению не подразумевает сходимость плотностей вероятности. Например, можно рассмотреть $X_n \in [0, 1]$ с функцией распределения $f_{X_n} = 1 - \cos(2\pi n x)$. Они сходятся по распределению к $U(0, 1)$, но их плотности вообще не сходятся. Но обратное верно: если плотности сходятся почти всюду, то почти всюду сходятся и функции распределения.

\emph{Пример:} запустили фабрику по выпуску костей $d6$. Но так как оборуждование новое и ненастроенное, кости получаются нечестными. Но со временем, они становятся все более и более честными, и их функция распределения сходится поточечно к функции распределения честной $d6$.

\textbf{Сходимость по вероятности.}

Говорят, что $X_n \to X$ по вероятности (все $X_n$ и $X$ -- с.в. на одном и том же вероятностном пространстве), если для любого $\eps > 0$
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.4\textwidth}
      \[
        \lim_{n \to +\infty} \Pr(|X - X_n| > \eps) = 0
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}
То есть в данном случае это означает, что $X_n$ с росто $n$ становится все более и более зависимой от $X$, точнее даже почти равной. Особый случай, когда $X$ --- константа. Тогда это означает, что последовательность $X_n$ становится все более и более сконцентрированной вокруг этой константы. 

Проведем аналогию с сходимостью последовательностей. Для последовательности $a_n \to a$ значит, что для любого чилсла $\eps > 0$ мы можем нарисовать $\eps$-трубку вокруг $a$, и в нее, начиная с какого-то $N$ попадут все члены последовательности. В случае, когда с.в. $X_n \to a$ по вероятнсоти, это значит, что для любого $\delta> 0$ начиная с какого-то $N$ почти вся вероятностная масса $X_n$ (а именно хотя бы $1 - \delta$) будет сконцентрирована в $\eps$-трубке. 

\begin{center}
  \begin{tikzpicture}
    \draw [<->, thick] (0, 4) -- (0, 0) -- (6, 0);
    \node [below] at (6, 0) {$n$};
    \node [left] at (0, 4) {$a_n$};
    
    \node [left] at (0, 2) {$a$};
    \node [left] at (0, 3) {$a + \eps$};
    \node [left] at (0, 1) {$a - \eps$};

    \draw [dashed] (0, 2) -- (6, 2);
    \draw [dotted] (0, 3) -- (6, 3);
    \draw [dotted] (0, 1) -- (6, 1);
    
    \draw (3, 0) -- (3, 4);
    \node [below] at (3, 0) {$N$};
    \draw [fill=blue] (0.5,0.5) circle (1mm);
    \draw [fill=blue] (1,2) circle (1mm);
    \draw [fill=blue] (1.5,3.5) circle (1mm);
    \draw [fill=blue] (2,3.1) circle (1mm);
    \draw [fill=blue] (2.5,0.8) circle (1mm);
    \draw [fill=blue] (3,1) circle (1mm); 
    \draw [fill=blue] (3.5,1.5) circle (1mm);
    \draw [fill=blue] (4,2) circle (1mm);
    \draw [fill=blue] (4.5,1.5) circle (1mm);
    \draw [fill=blue] (5,2.5) circle (1mm);
    \draw [fill=blue] (5.5,2.8) circle (1mm);
 

    \draw [<->, thick] (8, 4) -- (8, 0) -- (14, 0);
    \node [below] at (14, 0) {$n$};
    \node [left] at (8, 4) {$X_n$};

    \node [left] at (8, 2) {$a$};
    \node [left] at (8, 3) {$a + \eps$};
    \node [left] at (8, 1) {$a - \eps$};

    \draw [draw=none, fill=blue!50!white] (9, 4) to[bend right=20] (9.5, 3) -- (9, 3);
    \draw [draw=none, fill=blue!50!white] (9.5, 1) to[bend right=20] (9, 0) -- (9, 1);
    \draw (9, 0) -- (9, 4) to[bend right=20] (9.5, 3) to[bend left=45] (9.5, 1) to[bend right=20] (9, 0);

    \draw [draw=none, fill=green!50!white] (12, 4) to[bend right=5] (12.1, 3) -- (12, 3);
    \draw [draw=none, fill=green!50!white] (12.1, 1) to[bend right=5] (12, 0) -- (12, 1);
    \draw (12, 0) -- (12, 4) to[bend right=5] (12.1, 3) to[bend left=90] (12.1, 1) to[bend right=5] (12, 0);


    \draw [dashed] (8, 2) -- (14, 2);
    \draw [dotted] (8, 3) -- (14, 3);
    \draw [dotted] (8, 1) -- (14, 1);

  \end{tikzpicture}
\end{center}

Сходимость по вероятности подразумевает сходимость по распределению (доказать --- упражнение).

Важные свойства: 
\begin{itemize}
  \item Если $g(x)$ --- непрерывная функция, то $g(X_n)$ также сходятся к $g(X)$ по вероятности.
  \item Если $X_n \to X$ и $Y_n \to Y$, то $(X_n + Y_n) \to (X + Y)$ (везде сходимость по вероятности).
\end{itemize}

\emph{Пример:} вы смотрите на то, какие числа выдает генератор псевдослучайных чисел. Пусть $X_n$ --- ваше предположение о том, что он выдаст следующим числом после того, как вы пронаблюдали уже $n$ числе, выданных им. Чем больше $n$, тем больше вероятность правильно угадать паттерн, по которому действует генератор. Поэтому вероятность, что $X_n = X$ (где $X$ --- число, выданное генератором после вашего предположения), стремится к 1.

\emph{Еще пример:} Пусть $X_i$ -- независимые с.в., все $\sim U(0, 1)$. Пусть $Y_n = \min_{i \in [1..n]}(X_i)$. Тогда $\Pr(|Y_n - 0| \ge \eps) = \eps^n \to 0$. 

\emph{NB:} сходимость по вероятности не подразумевает сходимость матожиданий. Пусть $X_n$ такие, что
\begin{align*}
  \Pr(X_n = 0) &= 1 - \frac{1}{n} \\
  \Pr(X_n = n^2) &= \frac 1n
\end{align*}
Тогда $X_n \to 0$ по вероятности, но $E[X_n] = n^2 \cdot \frac 1n = n$ --- расходится.

\textbf{Сходимость почти наверное.}
Говорят, что $X_n \to X$ по вероятности (все $X_n$ и $X$ -- с.в. на одном и том же вероятностном пространстве), если 
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.4\textwidth}
      \[
        \Pr(\lim_{n \to +\infty} X_n = X) = 1
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}
Это означает, что при всех возможных элементарных исходах из $\Omega$ (кроме, возможно, какого-то множество исходов с суммарной вероятностью ноль) $X_n$ образуют такую последовательность, которая сходится к значению $X$ на этом же элементарном исходе:
\begin{align*}
  \Pr\left(\{\omega: \lim_{n \to \infty} X_n(\omega) = X(\omega)\}\right) = 1.
\end{align*}

Это наиболее сильный вид сходимости, который влечет за собой сходимость по вероятности (и следовательно, сходимость по распределению). Доказать это --- тоже упражнение.

\emph{Пример:} Пусть продолжительность жизни какого-то животного --- случайная величина. Пусть $X_i$ --- количество еды, которое съело это животное в $i$-й день своей жизни. При любом исходе с какого-то момента $X_i$ станет нулем, то есть последовательность сходится к нулю почти наверное.

\section{Законы больших чисел}

\textbf{Слабый закон больших чисел.}

Пусть $X_i$ --- независимые одинаково распределенные с.в. с матожиданием $\mu$ и дисперсией $\sigma^2$. Пусть $M_n = \frac{1}{n} \sum_{i = 1}^n X_i$ --- среднее первых $n$ случайных величин. Тогда
\begin{itemize}
  \item $E[M_n] = E[\frac{1}{n} \sum_{i = 1}^n X_i] = \frac{1}{n} \cdot n\mu = \mu$
  \item $\Var(M_n) = \Var(\frac{1}{n} \sum_{i = 1}^n X_i) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$
\end{itemize}
То есть с ростом $n$ среднее значение $M_n$ сохраняется, а вот дисперсия уменьшается (то есть увеличивается концентрация вокруг матожидания). Поэтому давайте применим неравенство Чебышева:
\begin{align*}
  \Pr(|M_n - \mu| \ge \eps) \le \frac{\Var(M_n)}{\eps^2} = \frac{\sigma^2}{n\eps^2} \to 0.
\end{align*}
То есть $M_n$ сходится по вероятности к $\mu$, что и называется слабым законом больших чисел.
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.6\textwidth}
      \[
        \forall \eps > 0 \text{ выполняется } \lim_{n \to +\infty} \Pr(|M_n - \mu| > \eps) = 0
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}

\emph{Возможные применения.}

\begin{itemize}
  \item Для точного измерения при наличии шумов. Пусть хотим узнать какую-то величину $\mu$, но при ее измерении у нас всегда есть какая-то случайная погрешность $W$ (причем $E(W_i) = 0$). Тогда чем больше измерений мы проведем, тем меньше будет вероятность, что среднее наших измерений сильно отклоняется от реального значения.
  \item Среднее по экспериментам. Пусть есть нечестная монета с вероятностью $p$, что выпадет орел. Тогда чем больше экспериментов мы проведем, тем лучше сможем определить этот $p$. 
\end{itemize}

\textbf{Сильный закон больших чисел (закон Колмогорова).}

В тех же условиях, что и для слабого закона, верно и более сильное утверждение:
\begin{center}
  \begin{tikzpicture}[rounded corners]
    \node [draw, rectangle, fill=blue!20, minimum height = 1.5cm, minimum width = 4.5cm] at (0,0) {
    \begin{minipage}{0.4\textwidth}
      \[
        \Pr(\lim_{n \to +\infty} M_n = \mu) = 1
      \]
    \end{minipage}};
  \end{tikzpicture}
\end{center}
То есть $M_n$ сходится к $\mu$ почти наверное. Также существует версия и для случая, когда $X_i$ имеют разные распределения (но с конечными матожиданиями и дисперсиями такими, что $\sum_{i = 1}^{+\infty} \frac{1}{i^2} \Var(X_i)$ --- сходится). В таком случае $(M_n - E[M_n])$ сходится к нулю почти наверное.

\end{document}